# modules/pricePrediction/data.py

import pandas as pd
import numpy as np
import mysql.connector  # æ›¿æ¢sqlite3
from sklearn.linear_model import RidgeCV, LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
import logging
import json

logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

from .translation import (
    translate_table_name,
    translate_field_name,
    reverse_translate_table_name,
    PRICE_PREDICTION_TABLE_TRANSLATIONS
)

MYSQL_CONFIG = {
    'host': 'localhost',  # ä¿®æ”¹ä¸ºæ‚¨çš„MySQLæœåŠ¡å™¨åœ°å€
    'user': 'dash',  # ä¿®æ”¹ä¸ºæ‚¨çš„MySQLç”¨æˆ·å
    'password': '123456',  # ä¿®æ”¹ä¸ºæ‚¨çš„MySQLå¯†ç 
    'database': 'dash_project',  # ä¿®æ”¹ä¸ºæ‚¨çš„MySQLæ•°æ®åº“å
    'charset': 'utf8mb4',
    'autocommit': True  # é»˜è®¤å¯ç”¨è‡ªåŠ¨æäº¤
}


# ç®—æ³•åç§°æ˜ å°„é…ç½®
ALGORITHM_NAME_MAPPING = {
    # æ•°æ®åº“ä¸­çš„algorithm_name -> ä»£ç ä¸­ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†
    "çº¿æ€§å›å½’": "å²­å›å½’ (RidgeCV)",
    "ç¥ç»ç½‘ç»œ": "ç¥ç»ç½‘ç»œ (MLPRegressor)",
    "å†³ç­–æ ‘": "å†³ç­–æ ‘ (Decision Tree)",
    "éšæœºæ£®æ—": "éšæœºæ£®æ— (Random Forest)",
    "æ”¯æŒå‘é‡æœº": "æ”¯æŒå‘é‡å›å½’ (SVR)"
}

# åå‘æ˜ å°„ï¼šä»£ç æ ‡è¯† -> æ•°æ®åº“åç§°
REVERSE_ALGORITHM_MAPPING = {v: k for k, v in ALGORITHM_NAME_MAPPING.items()}

def get_db_algorithm_name(model_key):
    """æ ¹æ®ä»£ç ä¸­çš„æ¨¡å‹æ ‡è¯†è·å–æ•°æ®åº“ä¸­çš„ç®—æ³•åç§°"""
    # ç›´æ¥æ˜ å°„
    db_name = REVERSE_ALGORITHM_MAPPING.get(model_key)
    if db_name:
        return db_name
    
    # å°è¯•æ¨¡ç³ŠåŒ¹é…
    model_key_lower = model_key.lower()
    if "ridge" in model_key_lower or "çº¿æ€§" in model_key:
        return "çº¿æ€§å›å½’"
    elif "mlp" in model_key_lower or "neural" in model_key_lower or "ç¥ç»" in model_key:
        return "ç¥ç»ç½‘ç»œ"
    elif "decision" in model_key_lower or "tree" in model_key_lower or "å†³ç­–" in model_key:
        return "å†³ç­–æ ‘"
    elif "forest" in model_key_lower or "random" in model_key_lower or "éšæœº" in model_key:
        return "éšæœºæ£®æ—"
    elif "svr" in model_key_lower or "svm" in model_key_lower or "æ”¯æŒ" in model_key:
        return "æ”¯æŒå‘é‡æœº"
    
    return None

def get_model_key_from_db_name(db_name):
    """
    æ ¹æ®æ•°æ®åº“ç®—æ³•åç§°è·å–ä»£ç ä¸­çš„æ¨¡å‹æ ‡è¯†
    """
    return ALGORITHM_NAME_MAPPING.get(db_name)


def get_connection():
    """è·å–MySQLæ•°æ®åº“è¿æ¥"""
    try:
        conn = mysql.connector.connect(**MYSQL_CONFIG)
        return conn
    except Exception as e:
        print(f"MySQLè¿æ¥å¤±è´¥: {e}")
        raise

# å¯¼å…¥é…ç½®
from .config import (
    STEEL_CAGE_COL_MAPPING, STEEL_CAGE_ML_FEATURES,
    STEEL_LINING_COL_MAPPING, STEEL_LINING_ML_FEATURES
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class CostPredictionSystem:
    """é’¢ç­‹ç¬¼/é’¢è¡¬é‡Œæ–½å·¥æˆæœ¬é¢„æµ‹ç³»ç»Ÿ"""

    def __init__(self, mode='steel_cage'):
        """
        åˆå§‹åŒ–é¢„æµ‹ç³»ç»Ÿ

        Parameters:
        - mode: 'steel_cage' (é’¢ç­‹ç¬¼æ¨¡å¼) æˆ– 'steel_lining' (é’¢è¡¬é‡Œæ¨¡å¼)
        """
        self.mode = mode
        self.df_historical = None
        self.kmeans = None
        self.scaler_cluster = None
        self.cluster_rules = {}
        self.global_rules = {}
        self.use_clustering = True
        self.n_clusters = 2

        # æœºå™¨å­¦ä¹ æ¨¡å‹
        self.models = {}
        self.is_trained = False
        self.logger = logging.getLogger(f"{self.__class__.__name__}-{self.mode}")

        # æ ¹æ®æ¨¡å¼é€‰æ‹©æ•°æ®åº“è¡¨å’Œç‰¹å¾åˆ—
        if self.mode == 'steel_cage':
            # é’¢ç­‹ç¬¼æ¨¡å¼ä½¿ç”¨MySQLè¡¨
            self.key_factors_table = 'key_factors_1'
            self.price_table = 'price_baseline_1'
            self.col_mapping = STEEL_CAGE_COL_MAPPING
            self.ml_features = STEEL_CAGE_ML_FEATURES
            self.target_column = 'é¡¹ç›®æ€»ä»·'
            self.core_quantity_key = 'é’¢ç­‹æ€»å¨æ•°'
            self.cluster_features_for_matching = ['é’¢ç­‹æ€»å¨æ•°', 'å¥—ç­’æ•°é‡']
            self.ratio_method_factor_col = 'äº”å¤§å› ç´ å®é™…æˆæœ¬å æ€»æˆæœ¬çš„æ¯”ä¾‹'

            self.ml_feature_to_qty_map = {
                'æˆæœ¬_å¡”åŠç§Ÿèµ': 'å¡”åŠç§Ÿèµå·¥ç¨‹é‡',
                'æˆæœ¬_é’¢ç­‹ç”Ÿäº§çº¿': 'é’¢ç­‹æ€»å¨æ•°',
                'æˆæœ¬_åŠç´¢å…·': 'åŠç´¢å…·æ•°é‡',
                'æˆæœ¬_å¥—ç­’': 'å¥—ç­’æ•°é‡',
            }

        elif self.mode == 'steel_lining':
            # é’¢è¡¬é‡Œæ¨¡å¼ä½¿ç”¨MySQLè¡¨
            self.key_factors_table = 'key_factors_2'
            self.price_table = 'price_baseline_2'
            self.col_mapping = STEEL_LINING_COL_MAPPING
            self.ml_features = STEEL_LINING_ML_FEATURES
            self.target_column = 'é¡¹ç›®æ€»ä»·'
            self.core_quantity_key = 'æ‹¼è£…åœºåœ°æ€»å·¥ç¨‹é‡'
            self.cluster_features_for_matching = ['æ‹¼è£…åœºåœ°æ€»å·¥ç¨‹é‡', 'åˆ¶ä½œèƒå…·æ€»å·¥ç¨‹é‡']
            self.ratio_method_factor_col = 'å…­å¤§å› ç´ å®é™…æˆæœ¬å æ€»æˆæœ¬çš„æ¯”ä¾‹'

            self.ml_feature_to_qty_map = {
                'æ‹¼è£…åœºåœ°è´¹ç”¨': 'æ‹¼è£…åœºåœ°æ€»å·¥ç¨‹é‡',
                'åˆ¶ä½œèƒå…·è´¹ç”¨': 'åˆ¶ä½œèƒå…·æ€»å·¥ç¨‹é‡',
                'é’¢æ”¯å¢©ã€åŸ‹ä»¶è´¹ç”¨': 'é’¢æ”¯å¢©åŸ‹ä»¶æ€»å·¥ç¨‹é‡',
                'æ‰¶å£æŸ±è´¹ç”¨': 'æ‰¶å£æŸ±æ€»å·¥ç¨‹é‡',
                'èµ°é“æ¿åŠæ“ä½œå¹³å°è´¹ç”¨': 'èµ°é“æ¿æ“ä½œå¹³å°æ€»å·¥ç¨‹é‡',
                'é’¢ç½‘æ¶è´¹ç”¨': 'é’¢ç½‘æ¢æ€»å·¥ç¨‹é‡',
            }
        else:
            raise ValueError(f"Unsupported mode: {mode}")
       
        # æ–°å¢ï¼šç®—æ³•çŠ¶æ€ç›¸å…³å±æ€§
        self.enabled_algorithms = {}  # å­˜å‚¨å¯ç”¨çš„ç®—æ³•ä¿¡æ¯
        self.algorithm_configs = {}   # å­˜å‚¨ç®—æ³•é…ç½®ä¿¡æ¯
        self.algorithm_status_loaded = False
        # åˆå§‹åŒ–ç»¼åˆæŒ‡æ ‡çŠ¶æ€ç›¸å…³å±æ€§
        self.comprehensive_indicators_status = {}
        self.prediction_method_availability = {}
        # æ–°å¢ï¼šç®—æ³•å‚æ•°ç¼“å­˜
        self.algorithm_parameters_cache = {}
        self.parameters_loaded = False


    def load_algorithm_parameters_from_db(self):
        """
        ä»algorithm_parametersè¡¨åŠ è½½æ‰€æœ‰ç®—æ³•å‚æ•°
        
        Returns:
            bool: åŠ è½½æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        conn = None
        try:
            conn = get_connection()
            cursor = conn.cursor(dictionary=True)
            
            # æŸ¥è¯¢æ‰€æœ‰ç®—æ³•å‚æ•°
            query = """
            SELECT 
                algorithm_name,
                parameter_name,
                current_value,
                parameter_type
            FROM algorithm_parameters 
            ORDER BY algorithm_name, parameter_name
            """
            
            cursor.execute(query)
            results = cursor.fetchall()
            
            # æŒ‰ç®—æ³•åç§°ç»„ç»‡å‚æ•°
            self.algorithm_parameters_cache = {}
            
            for row in results:
                algorithm_name = row['algorithm_name']
                
                if algorithm_name not in self.algorithm_parameters_cache:
                    self.algorithm_parameters_cache[algorithm_name] = {}
                
                # æš‚æ—¶åªå­˜å‚¨åŸå§‹å€¼ï¼Œä¸åšå¤æ‚è§£æ
                self.algorithm_parameters_cache[algorithm_name][row['parameter_name']] = {
                    'value': row['current_value'],
                    'type': row['parameter_type']
                }
            
            self.parameters_loaded = True
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(self.algorithm_parameters_cache)} ä¸ªç®—æ³•çš„å‚æ•°é…ç½®")
            
            # æ‰“å°åŠ è½½çš„å‚æ•°ä¿¡æ¯ç”¨äºè°ƒè¯•
            for alg_name, params in self.algorithm_parameters_cache.items():
                param_count = len(params)
                self.logger.info(f"  - {alg_name}: {param_count} ä¸ªå‚æ•°")
                
            return True
            
        except mysql.connector.Error as e:
            self.logger.error(f"ä»æ•°æ®åº“åŠ è½½ç®—æ³•å‚æ•°å¤±è´¥: {e}")
            return False
        except Exception as e:
            self.logger.error(f"åŠ è½½ç®—æ³•å‚æ•°æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
        finally:
            if conn:
                conn.close()

    def get_algorithm_parameters_raw(self, algorithm_name):
        """
        è·å–æŒ‡å®šç®—æ³•çš„åŸå§‹å‚æ•°æ•°æ®ï¼ˆä¸åšè§£æï¼‰
        
        Args:
            algorithm_name (str): ç®—æ³•åç§°ï¼ˆä¸­æ–‡ï¼‰
            
        Returns:
            dict: åŸå§‹å‚æ•°æ•°æ®
        """
        # å¦‚æœå‚æ•°è¿˜æœªåŠ è½½ï¼Œå…ˆåŠ è½½
        if not self.parameters_loaded:
            self.load_algorithm_parameters_from_db()
        
        # ä»ç¼“å­˜ä¸­è·å–å‚æ•°
        if algorithm_name in self.algorithm_parameters_cache:
            return self.algorithm_parameters_cache[algorithm_name]
        else:
            self.logger.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç®—æ³• '{algorithm_name}' çš„å‚æ•°é…ç½®")
            return {}

    def test_parameter_loading(self):
        """
        æµ‹è¯•å‚æ•°åŠ è½½åŠŸèƒ½çš„ç®€å•æ–¹æ³•
        """
        print("ğŸ”§ æµ‹è¯•å‚æ•°åŠ è½½...")
        
        success = self.load_algorithm_parameters_from_db()
        print(f"åŠ è½½ç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        
        if success:
            print(f"ğŸ“Š åŠ è½½äº† {len(self.algorithm_parameters_cache)} ä¸ªç®—æ³•çš„å‚æ•°")
            
            # æ˜¾ç¤ºæ¯ä¸ªç®—æ³•çš„å‚æ•°
            for alg_name, params in self.algorithm_parameters_cache.items():
                print(f"\nğŸ”¹ {alg_name}:")
                for param_name, param_info in params.items():
                    print(f"  - {param_name}: {param_info['value']} ({param_info['type']})")
        
        return success

    def _parse_parameter_value(self, value_str, param_type):
        """
        æ ¹æ®å‚æ•°ç±»å‹è§£æå‚æ•°å€¼
        
        Args:
            value_str (str): æ•°æ®åº“ä¸­å­˜å‚¨çš„å‚æ•°å€¼å­—ç¬¦ä¸²
            param_type (str): å‚æ•°ç±»å‹ ('continuous', 'discrete', 'categorical')
            
        Returns:
            è§£æåçš„å‚æ•°å€¼
        """
        if value_str is None or value_str == 'None':
            return None
            
        try:
            if param_type == 'categorical':
                # åˆ†ç±»å‚æ•°ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²
                return str(value_str)
                
            elif param_type == 'discrete':
                # ç¦»æ•£å‚æ•°ï¼Œè½¬æ¢ä¸ºæ•´æ•°
                if value_str == 'None (è‡ªåŠ¨)':
                    return None
                return int(float(value_str))
                
            elif param_type == 'continuous':
                # è¿ç»­å‚æ•°ï¼Œå¤„ç†ç‰¹æ®Šæ ¼å¼
                if 'np.logspace' in value_str:
                    # è§£æ np.logspace(-6, 6, 13) æ ¼å¼
                    import re
                    import numpy as np
                    match = re.search(r'np\.logspace\(([^)]+)\)', value_str)
                    if match:
                        params = [float(x.strip()) for x in match.group(1).split(',')]
                        return np.logspace(params[0], params[1], int(params[2]))
                elif value_str.startswith('(') and value_str.endswith(')'):
                    # è§£æå…ƒç»„æ ¼å¼ (3,) -> [3]
                    content = value_str[1:-1].strip()
                    if content.endswith(','):
                        content = content[:-1]
                    if ',' in content:
                        return [int(x.strip()) for x in content.split(',')]
                    else:
                        return [int(content)] if content else []
                else:
                    # æ™®é€šæ•°å€¼
                    return float(value_str)
                    
        except (ValueError, TypeError) as e:
            self.logger.warning(f"è§£æå‚æ•°å€¼å¤±è´¥ '{value_str}' (ç±»å‹: {param_type}): {e}")
            return value_str  # è¿”å›åŸå§‹å­—ç¬¦ä¸²
            
        return value_str

    def get_algorithm_parameters_parsed(self, algorithm_name):
        """
        è·å–æŒ‡å®šç®—æ³•çš„è§£æåå‚æ•°é…ç½®
        
        Args:
            algorithm_name (str): ç®—æ³•åç§°ï¼ˆä¸­æ–‡ï¼‰
            
        Returns:
            dict: è§£æåçš„å‚æ•°å­—å…¸ï¼Œé”®ä¸ºå‚æ•°åï¼Œå€¼ä¸ºè§£æåçš„å‚æ•°å€¼
        """
        # è·å–åŸå§‹å‚æ•°æ•°æ®
        raw_params = self.get_algorithm_parameters_raw(algorithm_name)
        
        if not raw_params:
            return {}
        
        # è§£æå‚æ•°å€¼
        parsed_params = {}
        for param_name, param_info in raw_params.items():
            parsed_value = self._parse_parameter_value(
                param_info['value'], 
                param_info['type']
            )
            parsed_params[param_name] = parsed_value
            
            # è®°å½•è§£æç»“æœ
            self.logger.debug(f"{algorithm_name}.{param_name}: '{param_info['value']}' -> {parsed_value}")
        
        return parsed_params

    def test_parameter_parsing(self):
        """
        æµ‹è¯•å‚æ•°è§£æåŠŸèƒ½
        """
        print("\nğŸ”„ æµ‹è¯•å‚æ•°è§£æåŠŸèƒ½:")
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            ("np.logspace(-6, 6, 13)", "continuous", "å²­å›å½’çš„alphaså‚æ•°"),
            ("(3,)", "continuous", "ç¥ç»ç½‘ç»œçš„éšè—å±‚ç»“æ„"), 
            ("linear", "categorical", "SVRçš„æ ¸å‡½æ•°ç±»å‹"),
            ("2", "discrete", "å†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦"),
            ("0.1", "continuous", "æ­£åˆ™åŒ–å‚æ•°"),
            ("None (è‡ªåŠ¨)", "discrete", "è‡ªåŠ¨è®¾ç½®å‚æ•°"),
            ("10", "discrete", "éšæœºæ£®æ—çš„æ ‘æ•°é‡")
        ]
        
        print("\nğŸ“‹ å‚æ•°è§£ææµ‹è¯•ç»“æœ:")
        for value_str, param_type, description in test_cases:
            try:
                parsed_value = self._parse_parameter_value(value_str, param_type)
                value_type = type(parsed_value).__name__
                print(f"  âœ… {description}:")
                print(f"     è¾“å…¥: '{value_str}' ({param_type})")
                print(f"     è¾“å‡º: {parsed_value} ({value_type})")
                
                # ç‰¹æ®Šæƒ…å†µçš„é¢å¤–ä¿¡æ¯
                if isinstance(parsed_value, np.ndarray):
                    print(f"     æ•°ç»„é•¿åº¦: {len(parsed_value)}, èŒƒå›´: [{parsed_value[0]:.2e}, {parsed_value[-1]:.2e}]")
                elif isinstance(parsed_value, list):
                    print(f"     åˆ—è¡¨å†…å®¹: {parsed_value}")
                    
            except Exception as e:
                print(f"  âŒ {description}: è§£æå¤±è´¥ - {e}")
            print()
        
        # æµ‹è¯•å®é™…ç®—æ³•å‚æ•°è§£æ
        print("ğŸ¯ æµ‹è¯•å®é™…ç®—æ³•å‚æ•°è§£æ:")
        
        if self.parameters_loaded:
            test_algorithms = ["å²­å›å½’", "å†³ç­–æ ‘", "éšæœºæ£®æ—"]
            
            for alg_name in test_algorithms:
                if alg_name in self.algorithm_parameters_cache:
                    print(f"\nğŸ“Š {alg_name}:")
                    
                    # è·å–åŸå§‹å‚æ•°
                    raw_params = self.get_algorithm_parameters_raw(alg_name)
                    print("  åŸå§‹å‚æ•°:")
                    for param_name, param_info in raw_params.items():
                        print(f"    {param_name}: '{param_info['value']}' ({param_info['type']})")
                    
                    # è·å–è§£æåå‚æ•°
                    parsed_params = self.get_algorithm_parameters_parsed(alg_name)
                    print("  è§£æåå‚æ•°:")
                    for param_name, parsed_value in parsed_params.items():
                        print(f"    {param_name}: {parsed_value} ({type(parsed_value).__name__})")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°ç®—æ³• '{alg_name}' çš„å‚æ•°")

    def compare_with_default_params(self):
        """
        æ¯”è¾ƒæ•°æ®åº“å‚æ•°ä¸é»˜è®¤å‚æ•°çš„å·®å¼‚
        """
        print("\nğŸ” æ•°æ®åº“å‚æ•°ä¸é»˜è®¤å‚æ•°å¯¹æ¯”:")
        
        # å®šä¹‰é»˜è®¤å‚æ•°ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        default_params = {
            "å²­å›å½’": {
                "alphas": "np.logspace(-6, 6, 13)",
                "cv": "None"
            },
            "å†³ç­–æ ‘": {
                "max_depth": 2,
                "min_samples_leaf": 2,
                "min_samples_split": 2
            },
            "éšæœºæ£®æ—": {
                "n_estimators": 10,
                "max_depth": 2,
                "max_features": "None"
            }
        }
        
        for alg_name, default_values in default_params.items():
            print(f"\nğŸ“Š {alg_name}:")
            
            # è·å–æ•°æ®åº“ä¸­çš„è§£æå‚æ•°
            db_params = self.get_algorithm_parameters_parsed(alg_name)
            
            for param_name, default_value in default_values.items():
                db_value = db_params.get(param_name, "âŒ æœªæ‰¾åˆ°")
                
                # ç®€å•æ¯”è¾ƒ
                if str(db_value) == str(default_value):
                    status = "âœ… ç›¸åŒ"
                else:
                    status = "ğŸ”„ ä¸åŒ"
                
                print(f"  {param_name}:")
                print(f"    é»˜è®¤å€¼: {default_value}")
                print(f"    æ•°æ®åº“: {db_value} {status}")

    def _create_models_with_db_params(self):
        """
        ä½¿ç”¨æ•°æ®åº“å‚æ•°é…ç½®åˆ›å»ºæ¨¡å‹
        
        Returns:
            dict: é…ç½®å¥½çš„æ¨¡å‹å­—å…¸
        """
        models_config = {}
        
        # å²­å›å½’
        ridge_params = self.get_algorithm_parameters_parsed("å²­å›å½’")
        
        # å¤„ç†alphaså‚æ•°
        alphas = ridge_params.get('alphas', np.logspace(-6, 6, 13))
        cv_folds = ridge_params.get('cv', None)
        
        models_config["å²­å›å½’ (RidgeCV)"] = Pipeline([
            ('scaler', StandardScaler()), 
            ('regressor', RidgeCV(alphas=alphas, cv=cv_folds))
        ])
        
        self.logger.info(f"å²­å›å½’å‚æ•° - alphas: {type(alphas).__name__}({len(alphas) if hasattr(alphas, '__len__') else 'scalar'}), cv: {cv_folds}")
        
        # å†³ç­–æ ‘
        tree_params = self.get_algorithm_parameters_parsed("å†³ç­–æ ‘")
        
        models_config["å†³ç­–æ ‘ (Decision Tree)"] = Pipeline([
            ('regressor', DecisionTreeRegressor(
                random_state=42,
                max_depth=tree_params.get('max_depth', 2),
                min_samples_leaf=tree_params.get('min_samples_leaf', 2),
                min_samples_split=tree_params.get('min_samples_split', 2)
            ))
        ])
        
        self.logger.info(f"å†³ç­–æ ‘å‚æ•° - max_depth: {tree_params.get('max_depth', 2)}, min_samples_leaf: {tree_params.get('min_samples_leaf', 2)}")
        
        # éšæœºæ£®æ—
        forest_params = self.get_algorithm_parameters_parsed("éšæœºæ£®æ—")
        
        # å¤„ç†max_featureså‚æ•°ï¼ˆå¯èƒ½æ˜¯Noneï¼‰
        max_features = forest_params.get('max_features', None)
        if max_features == "None (å…¨éƒ¨ç‰¹å¾)":
            max_features = None
        
        models_config["éšæœºæ£®æ— (Random Forest)"] = Pipeline([
            ('regressor', RandomForestRegressor(
                random_state=42,
                n_estimators=forest_params.get('n_estimators', 10),
                max_depth=forest_params.get('max_depth', 2),
                max_features=max_features
            ))
        ])
        
        self.logger.info(f"éšæœºæ£®æ—å‚æ•° - n_estimators: {forest_params.get('n_estimators', 10)}, max_features: {max_features}")
        
        # æ”¯æŒå‘é‡å›å½’
        svr_params = self.get_algorithm_parameters_parsed("æ”¯æŒå‘é‡å›å½’")
        
        models_config["æ”¯æŒå‘é‡å›å½’ (SVR)"] = Pipeline([
            ('scaler', StandardScaler()), 
            ('regressor', SVR(
                C=svr_params.get('C', 0.1),
                kernel=svr_params.get('kernel', 'linear'),
                epsilon=svr_params.get('epsilon', 0.1)
            ))
        ])
        
        self.logger.info(f"SVRå‚æ•° - C: {svr_params.get('C', 0.1)}, kernel: {svr_params.get('kernel', 'linear')}")
        
        # ç¥ç»ç½‘ç»œ
        nn_params = self.get_algorithm_parameters_parsed("ç¥ç»ç½‘ç»œ")
        
        # å¤„ç†hidden_layer_sizeså‚æ•°
        hidden_layers = nn_params.get('hidden_layer_sizes', [3])
        
        # ç¡®ä¿hidden_layer_sizesæ˜¯å…ƒç»„æ ¼å¼ï¼ˆsklearnè¦æ±‚ï¼‰
        if isinstance(hidden_layers, list):
            hidden_layers = tuple(hidden_layers)
        elif isinstance(hidden_layers, int):
            hidden_layers = (hidden_layers,)
        elif not isinstance(hidden_layers, tuple):
            hidden_layers = (3,)  # é»˜è®¤å€¼
            
        models_config["ç¥ç»ç½‘ç»œ (MLPRegressor)"] = Pipeline([
            ('scaler', StandardScaler()), 
            ('regressor', MLPRegressor(
                hidden_layer_sizes=hidden_layers,
                activation='relu',
                solver='lbfgs',
                max_iter=nn_params.get('max_iter', 2000),
                random_state=42,
                alpha=nn_params.get('alpha', 0.1)
            ))
        ])
        
        self.logger.info(f"ç¥ç»ç½‘ç»œå‚æ•° - hidden_layers: {hidden_layers}, alpha: {nn_params.get('alpha', 0.1)}")
        
        self.logger.info(f"âœ… ä½¿ç”¨æ•°æ®åº“å‚æ•°é…ç½®åˆ›å»ºäº† {len(models_config)} ä¸ªæ¨¡å‹")
        return models_config

    def load_algorithm_configs(self):
        """ä»æ•°æ®åº“åŠ è½½ç®—æ³•é…ç½®ä¿¡æ¯"""
        conn = None
        try:
            conn = get_connection()
            cursor = conn.cursor(dictionary=True)
            
            # æŸ¥è¯¢å½“å‰æ¨¡å¼ä¸‹çš„ç®—æ³•é…ç½®
            query = """
            SELECT algorithm_name, status, parameters, model_description, application_scenario
            FROM algorithm_configs 
            WHERE construction_mode = %s OR construction_mode = 'general'
            ORDER BY algorithm_name
            """
            
            cursor.execute(query, (self.mode,))
            results = cursor.fetchall()
            
            self.algorithm_configs = {}
            self.enabled_algorithms = {}
            
            for row in results:
                algorithm_name = row['algorithm_name']
                self.algorithm_configs[algorithm_name] = {
                    'status': row['status'],
                    'parameters': row['parameters'],
                    'description': row['model_description'],
                    'scenario': row['application_scenario']
                }
                
                # å¦‚æœç®—æ³•å¯ç”¨ï¼ŒåŠ å…¥å¯ç”¨åˆ—è¡¨
                if row['status'] == 'enabled':
                    self.enabled_algorithms[algorithm_name] = True
                    
            self.algorithm_status_loaded = True
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(self.algorithm_configs)} ä¸ªç®—æ³•é…ç½®")
            self.logger.info(f"å¯ç”¨çš„ç®—æ³•: {list(self.enabled_algorithms.keys())}")
            
            return True
            
        except mysql.connector.Error as e:
            self.logger.error(f"åŠ è½½ç®—æ³•é…ç½®å¤±è´¥: {e}")
            return False
        except Exception as e:
            self.logger.error(f"åŠ è½½ç®—æ³•é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
        finally:
            if conn:
                conn.close()
    
    def get_enabled_algorithm_names(self):
        """è·å–å¯ç”¨çš„ç®—æ³•åç§°åˆ—è¡¨"""
        if not self.algorithm_status_loaded:
            self.load_algorithm_configs()
        return list(self.enabled_algorithms.keys())
    
    def is_algorithm_enabled(self, algorithm_name):
        """æ£€æŸ¥æŒ‡å®šç®—æ³•æ˜¯å¦å¯ç”¨"""
        if not self.algorithm_status_loaded:
            self.load_algorithm_configs()
        return self.enabled_algorithms.get(algorithm_name, False)
    
    def get_algorithm_config(self, algorithm_name):
        """è·å–æŒ‡å®šç®—æ³•çš„é…ç½®ä¿¡æ¯"""
        if not self.algorithm_status_loaded:
            self.load_algorithm_configs()
        return self.algorithm_configs.get(algorithm_name, {})
    
    def validate_algorithm_availability(self):
        """éªŒè¯ç®—æ³•å¯ç”¨æ€§"""
        if not self.algorithm_status_loaded:
            self.load_algorithm_configs()
            
        enabled_count = len(self.enabled_algorithms)
        
        if enabled_count == 0:
            return {
                'status': 'unavailable',
                'message': 'æ‰€æœ‰é¢„æµ‹ç®—æ³•éƒ½å·²åœç”¨ï¼Œæ— æ³•è¿›è¡Œæœºå™¨å­¦ä¹ é¢„æµ‹',
                'enabled_count': 0
            }
        elif enabled_count == 1:
            return {
                'status': 'limited',
                'message': f'ä»…æœ‰1ä¸ªç®—æ³•å¯ç”¨ï¼Œé¢„æµ‹å¯é æ€§è¾ƒä½ã€‚å¯ç”¨ç®—æ³•: {list(self.enabled_algorithms.keys())[0]}',
                'enabled_count': 1
            }
        else:
            return {
                'status': 'available', 
                'message': f'æœ‰{enabled_count}ä¸ªç®—æ³•å¯ç”¨ï¼Œé¢„æµ‹åŠŸèƒ½æ­£å¸¸',
                'enabled_count': enabled_count
            }


    def get_algorithm_status_info(self):
        """è·å–ç®—æ³•çŠ¶æ€ä¿¡æ¯ç”¨äºæ˜¾ç¤º"""
        if not self.algorithm_status_loaded:
            self.load_algorithm_configs()
            
        status_info = {
            'enabled': [],
            'disabled': [],
            'total_count': 0,
            'enabled_count': 0
        }
        
        # æŒ‰ç…§æ˜¾ç¤ºé¡ºåºæ•´ç†ç®—æ³•çŠ¶æ€
        display_order = [
            "å²­å›å½’ (RidgeCV)",
            "å†³ç­–æ ‘ (Decision Tree)", 
            "éšæœºæ£®æ— (Random Forest)",
            "æ”¯æŒå‘é‡å›å½’ (SVR)",
            "ç¥ç»ç½‘ç»œ (MLPRegressor)"
        ]
        
        for model_key in display_order:
            db_algorithm_name = get_db_algorithm_name(model_key)
            if db_algorithm_name:
                config = self.get_algorithm_config(db_algorithm_name)
                if config:  # åªå¤„ç†åœ¨æ•°æ®åº“ä¸­å­˜åœ¨çš„ç®—æ³•
                    status_info['total_count'] += 1
                    
                    algorithm_info = {
                        'display_name': model_key,
                        'db_name': db_algorithm_name,
                        'status': config.get('status', 'unknown'),
                        'description': config.get('description', ''),
                        'trained': model_key in self.models
                    }
                    
                    if config.get('status') == 'enabled':
                        status_info['enabled'].append(algorithm_info)
                        status_info['enabled_count'] += 1
                    else:
                        status_info['disabled'].append(algorithm_info)
        
        return status_info

    def find_best_algorithm_prediction(self, ml_predictions, ratio_prediction_value):
        """
        æ‰¾åˆ°æœ€ä½³ç®—æ³•é¢„æµ‹ - æ”¹è¿›ç‰ˆæœ¬ï¼Œåªä»å¯ç”¨çš„ç®—æ³•ä¸­é€‰æ‹©
        ä¿æŒåŸæœ‰çš„é€‰æ‹©é€»è¾‘ï¼šæœ€æ¥è¿‘æ¯”ç‡æ³•çš„ç®—æ³•
        """
        if not ratio_prediction_value or not ml_predictions:
            return None, None
            
        best_algorithm_key = None
        min_diff_to_ratio = float('inf')
        
        # åªè€ƒè™‘å¯ç”¨ä¸”æœ‰æœ‰æ•ˆé¢„æµ‹ç»“æœçš„ç®—æ³•
        for model_key, prediction_value in ml_predictions.items():
            # è·³è¿‡ç‰¹æ®Šé”®å’Œåœç”¨ç®—æ³•
            if (model_key in ['é›†æˆå¹³å‡é¢„æµ‹', '_algorithm_status'] or 
                prediction_value is None or 
                isinstance(prediction_value, dict)):  # åœç”¨ç®—æ³•è¿”å›å­—å…¸
                continue
                
            # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ªå¯ç”¨çš„ç®—æ³•ä¸”æœ‰æ•°å€¼é¢„æµ‹ç»“æœ
            if isinstance(prediction_value, (int, float)):
                diff = abs(prediction_value - ratio_prediction_value)
                if diff < min_diff_to_ratio:
                    min_diff_to_ratio = diff
                    best_algorithm_key = model_key
        
        if best_algorithm_key:
            best_prediction_value = ml_predictions[best_algorithm_key]
            self.logger.info(f"ğŸ† æœ€ä½³ç®—æ³•: {best_algorithm_key}, é¢„æµ‹å€¼: {best_prediction_value:.2f}")
            return best_algorithm_key, best_prediction_value
        else:
            self.logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä½³ç®—æ³•é¢„æµ‹")
            return None, None
        
    def load_data_from_database(self, mode, table_name=None):
        """ä»MySQLæ•°æ®åº“åŠ è½½æ•°æ®"""
        if table_name is None:
            if mode == 'steel_cage':
                table_name = "key_factors_1"
            elif mode == 'steel_lining':
                table_name = "key_factors_2"
        
        conn = None
        try:
            # ä½¿ç”¨MySQLè¿æ¥æ›¿æ¢SQLite
            conn = get_connection()
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæŸ¥çœ‹è¡¨ç»“æ„
            self.logger.info(f"æ­£åœ¨åŠ è½½è¡¨: {table_name}")
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨å¹¶æŸ¥çœ‹ç»“æ„
            try:
                structure_query = f"DESCRIBE `{table_name}`"
                df_structure = pd.read_sql_query(structure_query, conn)
                self.logger.info(f"è¡¨ {table_name} çš„åˆ—ç»“æ„:\n{df_structure[['Field', 'Type']].to_string()}")
            except Exception as e:
                self.logger.warning(f"æ— æ³•è·å–è¡¨ç»“æ„: {e}")
            
            query = f"SELECT * FROM `{table_name}`"
            df_raw = pd.read_sql_query(query, conn)
            
            query = f"SELECT * FROM `{table_name}`"
            df_raw = pd.read_sql_query(query, conn)
            
            # æ·»åŠ è¿™äº›è°ƒè¯•ä¿¡æ¯
            print(f"\n=== è°ƒè¯•ä¿¡æ¯ for {mode} ===")
            print(f"æŸ¥è¯¢çš„è¡¨å: {table_name}")
            print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df_raw.shape}")
            print(f"åŸå§‹åˆ—å: {list(df_raw.columns)}")
            print(f"å‰å‡ è¡Œæ•°æ®:")
            print(df_raw.head())
            print(f"é…ç½®çš„åˆ—åæ˜ å°„: {self.col_mapping}")

            
            self.logger.info(f"ä»è¡¨ {table_name} åŠ è½½äº† {len(df_raw)} è¡Œæ•°æ®")
            self.logger.info(f"åŸå§‹åˆ—å: {list(df_raw.columns)}")
            
            if len(df_raw) > 0:
                self.logger.info(f"æ•°æ®é¢„è§ˆ:\n{df_raw.head()}")
            else:
                self.logger.warning(f"è¡¨ {table_name} ä¸­æ²¡æœ‰æ•°æ®ï¼")
            
            self.df_historical = self._preprocess_data(df_raw)
            self.logger.info(f"æˆåŠŸä»MySQLæ•°æ®åº“åŠ è½½ {len(self.df_historical)} æ¡å†å²é¡¹ç›®æ•°æ® for {self.mode}")
            return True

        except mysql.connector.Error as e:
            self.logger.error(f"MySQLæ•°æ®åº“è¯»å–å¤±è´¥ for {self.mode}: {e}")
            self.df_historical = pd.DataFrame()
            return False
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¯»å–å¤±è´¥ for {self.mode}: {e}")
            self.df_historical = pd.DataFrame()
            return False
        finally:
            if conn:
                conn.close()


    def _preprocess_data(self, df_raw):
        df_processed = df_raw.copy()

        # æ ¹æ®æ¨¡å¼é‡å‘½ååˆ—
        df_processed.rename(columns=self.col_mapping, inplace=True)

        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æ•°å€¼åˆ—
        all_numeric_cols = list(self.ml_features) + [self.target_column] + self.cluster_features_for_matching

        # ä»·æ ¼åŸºå‡†ä¸­çš„æ•°å€¼åˆ—
        potential_price_db_numeric_cols = [
            "modular_labor_unit_price", "modular_labor_quantity", "modular_labor_total",
            "modular_material_unit_price", "modular_material_quantity", "modular_material_total",
            "modular_machinery_unit_price", "modular_machinery_quantity", "modular_machinery_total",
            "total_price"
        ]
        all_numeric_cols.extend(potential_price_db_numeric_cols)
        all_numeric_cols = list(set(all_numeric_cols))

        self.logger.debug(f"å°è¯•å°†ä»¥ä¸‹åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ for {self.mode}: {all_numeric_cols}")

        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„åˆ—å­˜åœ¨ä¸”ä¸ºæ•°å€¼ç±»å‹
        for col in all_numeric_cols:
            if col in df_processed.columns:
                original_dtype = df_processed[col].dtype
                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')
                if df_processed[col].dtype == object and original_dtype != object:
                    self.logger.warning(f"è­¦å‘Š: åˆ— '{col}' åœ¨ {self.mode} ä¸­è½¬æ¢ä¸ºæ•°å€¼å¤±è´¥ï¼Œå¯èƒ½åŒ…å«éæ•°å€¼å­—ç¬¦ã€‚")
            else:
                if col in (list(self.ml_features) + [self.target_column] + self.cluster_features_for_matching):
                    self.logger.warning(f"è­¦å‘Šï¼šå¤„ç†æ•°æ®æ—¶ï¼Œ'{col}' å­—æ®µåœ¨åŸå§‹æ•°æ®ä¸­ç¼ºå¤± for {self.mode}ã€‚å°†å¡«å……ä¸º0ã€‚")
                    df_processed[col] = 0.0

        # è¿‡æ»¤æ‰é¡¹ç›®æ€»ä»·ä¸º NaN æˆ– 0 çš„è¡Œ
        df_processed.dropna(subset=[self.target_column], inplace=True)
        df_processed = df_processed[df_processed[self.target_column] > 0].copy()

        # è®°å½•å…³é”®åˆ—çš„æ¦‚å†µ
        self.logger.info(f"é¢„å¤„ç†å {self.mode} æ¨¡å¼ '{self.target_column}' ç»Ÿè®¡ä¿¡æ¯:\n{df_processed[self.target_column].describe()}")
        for feature in self.ml_features:
            if feature in df_processed.columns:
                self.logger.info(f"é¢„å¤„ç†å {self.mode} æ¨¡å¼ '{feature}' ç»Ÿè®¡ä¿¡æ¯:\n{df_processed[feature].describe()}")
            else:
                self.logger.warning(f"MLç‰¹å¾ '{feature}' åœ¨é¢„å¤„ç†åçš„DataFrameä¸­ç¼ºå¤± for {self.mode}ã€‚")

        # é‡æ–°æ£€æŸ¥MLç‰¹å¾ï¼Œç¡®ä¿å®ƒä»¬éƒ½æ˜¯æ•°å€¼å‹
        for col in self.ml_features:
            if col not in df_processed.columns:
                df_processed[col] = 0.0
            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce').fillna(0.0)

        if self.mode == 'steel_lining':
            # é’¢è¡¬é‡Œæ¨¡å¼ç‰¹æœ‰çš„æ¯”ä¾‹å› å­è®¡ç®—
            for feature in self.ml_features:
                if feature not in df_processed.columns:
                    df_processed[feature] = 0.0

            temp_core_costs_sum = df_processed[self.ml_features].sum(axis=1)
            print("!!!")
            print(df_processed)
            print("!!!")
            # é¿å…é™¤é›¶
            df_processed[self.ratio_method_factor_col] = temp_core_costs_sum / df_processed[self.target_column]
            df_processed[self.ratio_method_factor_col].replace([np.inf, -np.inf], np.nan, inplace=True)
            df_processed[self.ratio_method_factor_col].fillna(0.0, inplace=True)

        self.logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ for {self.mode}ï¼Œå‰©ä½™ {len(df_processed)} æ¡æœ‰æ•ˆæ ·æœ¬ã€‚")
        return df_processed

    def train_system(self):
        if self.df_historical is None or len(self.df_historical) == 0:
            self.logger.error(f"é”™è¯¯ï¼šæ²¡æœ‰å†å²æ•°æ®ï¼Œè¯·å…ˆåŠ è½½æ•°æ® for {self.mode}")
            return False

        try:
            # ã€æ–°å¢ã€‘æ­¥éª¤0ï¼šåŠ è½½ç®—æ³•å‚æ•°é…ç½®
            if not self.load_algorithm_parameters_from_db():
                self.logger.warning(f"ç®—æ³•å‚æ•°åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•° for {self.mode}")
            
            # æ–°å¢ï¼šåŠ è½½ç»¼åˆæŒ‡æ ‡çŠ¶æ€
            self.load_comprehensive_indicators_status()
            
            # æ­¥éª¤1ï¼šèšç±»åˆ†æ
            self._perform_clustering()

            # æ­¥éª¤2ï¼šå»ºç«‹è§„åˆ™åº“
            self._build_rules()

            # æ­¥éª¤3ï¼šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆç°åœ¨ä¼šä½¿ç”¨æ•°æ®åº“å‚æ•°ï¼‰
            self._train_ml_models()

            self.is_trained = True
            self.logger.info(f"ç³»ç»Ÿè®­ç»ƒå®Œæˆ for {self.mode}")
            return True

        except Exception as e:
            self.logger.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™ for {self.mode}: {e}", exc_info=True)
            self.is_trained = False
            return False

    # æ–°å¢ï¼šå‚æ•°é…ç½®å¯¹æ¯”æ–¹æ³•ï¼ˆç”¨äºéªŒè¯ï¼‰

    def log_parameter_comparison(self):
        """
        è®°å½•æ•°æ®åº“å‚æ•°ä¸é»˜è®¤å‚æ•°çš„å¯¹æ¯”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        """
        self.logger.info("=== ç®—æ³•å‚æ•°é…ç½®å¯¹æ¯” ===")
        
        # é»˜è®¤å‚æ•°ï¼ˆåŸæ¥ç¡¬ç¼–ç çš„å€¼ï¼‰
        default_configs = {
            "å²­å›å½’": {"alphas": "np.logspace(-6, 6, 13)", "cv": None},
            "å†³ç­–æ ‘": {"max_depth": 2, "min_samples_leaf": 2, "min_samples_split": 2},
            "éšæœºæ£®æ—": {"n_estimators": 10, "max_depth": 2, "max_features": None},
            "æ”¯æŒå‘é‡å›å½’": {"C": 0.1, "kernel": "linear", "epsilon": 0.1},
            "ç¥ç»ç½‘ç»œ": {"hidden_layer_sizes": (3,), "alpha": 0.1, "max_iter": 2000}
        }
        
        for alg_name, default_params in default_configs.items():
            db_params = self.get_algorithm_parameters_parsed(alg_name)
            
            self.logger.info(f"\nğŸ“Š {alg_name}:")
            self.logger.info(f"  é»˜è®¤é…ç½®: {default_params}")
            self.logger.info(f"  æ•°æ®åº“é…ç½®: {db_params}")
            
            # æ£€æŸ¥å…³é”®å‚æ•°æ˜¯å¦æœ‰å˜åŒ–
            for param_name, default_value in default_params.items():
                db_value = db_params.get(param_name, "æœªæ‰¾åˆ°")
                if str(db_value) != str(default_value):
                    self.logger.info(f"  ğŸ”„ {param_name}: {default_value} â†’ {db_value}")
                else:
                    self.logger.info(f"  âœ… {param_name}: ä¿æŒé»˜è®¤å€¼ {default_value}")

    def _perform_clustering(self):
        # èšç±»ç‰¹å¾éœ€è¦æ ¹æ®æ¨¡å¼è¿›è¡Œè°ƒæ•´
        cluster_features = [f for f in self.cluster_features_for_matching if f in self.df_historical.columns]

        if not cluster_features or len(self.df_historical) < self.n_clusters * 2:
            self.logger.warning(f"è­¦å‘Šï¼šæ•°æ®ç‚¹è¿‡å°‘æˆ–èšç±»ç‰¹å¾ç¼ºå¤± ({len(self.df_historical)}), æ— æ³•æœ‰æ•ˆèšç±» for {self.mode}ï¼Œå°†ä½¿ç”¨å…¨å±€å¹³å‡è§„åˆ™")
            self.use_clustering = False
            self.df_historical['cluster_label'] = 0
            return

        X_for_clustering = self.df_historical[cluster_features].copy()
        X_for_clustering.dropna(inplace=True)
        if X_for_clustering.empty:
            self.logger.warning(f"èšç±»æ•°æ®ä¸ºç©º after dropping NaNs for {self.mode}. Using global rules.")
            self.use_clustering = False
            self.df_historical['cluster_label'] = 0
            return

        # æ ‡å‡†åŒ–èšç±»ç‰¹å¾
        self.scaler_cluster = StandardScaler()
        X_scaled_for_clustering = self.scaler_cluster.fit_transform(X_for_clustering)

        # æ‰§è¡ŒKMeansèšç±»
        try:
            kmeans = KMeans(n_clusters=min(self.n_clusters, len(X_for_clustering)), random_state=42, n_init='auto')
            self.df_historical.loc[X_for_clustering.index, 'cluster_label'] = kmeans.fit_predict(X_scaled_for_clustering)
            self.kmeans = kmeans
            self.use_clustering = True
            cluster_counts = self.df_historical['cluster_label'].value_counts().to_dict()
            self.logger.info(f"èšç±»å®Œæˆ for {self.mode}ï¼šå°†é¡¹ç›®åˆ†ä¸º {kmeans.n_clusters} ä¸ªç°‡: {cluster_counts}")
        except Exception as e:
            self.logger.error(f"æ‰§è¡ŒKMeansèšç±»å‡ºé”™ for {self.mode}: {e}. å›é€€åˆ°å…¨å±€è§„åˆ™ã€‚", exc_info=True)
            self.use_clustering = False
            self.df_historical['cluster_label'] = 0

    def _build_rules(self):
        if self.df_historical is None or self.df_historical.empty:
            self.logger.warning(f"æ²¡æœ‰å†å²æ•°æ®ç”¨äºæ„å»ºè§„åˆ™ for {self.mode}")
            return

        # è®¡ç®—å…¨å±€å¹³å‡è§„åˆ™ï¼ˆä½œä¸ºåå¤‡ï¼‰
        self.global_rules = self._calculate_rules_for_df(self.df_historical)
        self.logger.debug(f"Global rules for {self.mode}: {json.dumps(self.global_rules, indent=2)}")

        # å¦‚æœä½¿ç”¨èšç±»ï¼Œä¸ºæ¯ä¸ªç°‡è®¡ç®—è§„åˆ™
        if self.use_clustering and 'cluster_label' in self.df_historical.columns:
            for cluster_id in self.df_historical['cluster_label'].unique():
                df_cluster = self.df_historical[self.df_historical['cluster_label'] == cluster_id]
                if len(df_cluster) > 0:
                    self.cluster_rules[cluster_id] = self._calculate_rules_for_df(df_cluster)
                    self.logger.debug(f"Cluster {cluster_id} rules for {self.mode}: {json.dumps(self.cluster_rules[cluster_id], indent=2)}")
                else:
                    self.cluster_rules[cluster_id] = self.global_rules.copy()
        else:
            self.cluster_rules[0] = self.global_rules.copy()

        self.logger.info(f"è§„åˆ™åº“å»ºç«‹å®Œæˆ for {self.mode}")

    def _calculate_rules_for_df(self, df):
        rules = {}
        # ç›®æ ‡æ€»ä»·å¹³å‡å€¼
        rules['avg_target_cost'] = df[self.target_column].mean()

        # æ¯”ç‡æ³•å› å­
        if self.ratio_method_factor_col in df.columns:
            valid_factor_data = df[self.ratio_method_factor_col].replace([np.inf, -np.inf], np.nan).dropna()
            rules[self.ratio_method_factor_col] = valid_factor_data.mean() if not valid_factor_data.empty else 0.0
        else:
            rules[self.ratio_method_factor_col] = 0.0

        # ä¸ºæ‰€æœ‰ç›¸å…³åˆ—æ·»åŠ å¹³å‡æ•°é‡
        all_relevant_quantity_columns = [
            'æ‹¼è£…åœºåœ°æ€»å·¥ç¨‹é‡', 'åˆ¶ä½œèƒå…·æ€»å·¥ç¨‹é‡', 'é’¢æ”¯å¢©åŸ‹ä»¶æ€»å·¥ç¨‹é‡', 'æ‰¶å£æŸ±æ€»å·¥ç¨‹é‡',
            'èµ°é“æ¿æ“ä½œå¹³å°æ€»å·¥ç¨‹é‡', 'é’¢ç½‘æ¢æ€»å·¥ç¨‹é‡',
            'é’¢ç­‹æ€»å¨æ•°', 'å¡”åŠç§Ÿèµå·¥ç¨‹é‡', 'åŠç´¢å…·æ•°é‡', 'å¥—ç­’æ•°é‡',
            'é’¢æ”¯å¢©åŸ‹ä»¶æ··å‡åœŸå‰”å‡¿æ€»å·¥ç¨‹é‡', 'é’¢æ”¯å¢©åŸ‹ä»¶æ··å‡åœŸå›å¡«æ€»å·¥ç¨‹é‡',
            'é’¢æ”¯å¢©åŸ‹ä»¶å®‰è£…æ€»å·¥ç¨‹é‡', 'é’¢æ”¯å¢©åŸ‹ä»¶åˆ¶ä½œæ€»å·¥ç¨‹é‡',
            'æ‰¶å£æŸ±å®‰è£…æ€»å·¥ç¨‹é‡', 'æ‰¶å£æŸ±æ‹†é™¤æ€»å·¥ç¨‹é‡', 'æ‰¶å£æŸ±æ„ä»¶ä½¿ç”¨æŠ˜æ—§æ€»å·¥ç¨‹é‡',
            'èµ°é“æ¿æ“ä½œå¹³å°åˆ¶ä½œæ€»å·¥ç¨‹é‡', 'èµ°é“æ¿æ“ä½œå¹³å°æ­è®¾æ€»å·¥ç¨‹é‡', 'èµ°é“æ¿æ“ä½œå¹³å°æ‹†é™¤æ€»å·¥ç¨‹é‡',
            'é’¢ç½‘æ¶åˆ¶ä½œæ€»å·¥ç¨‹é‡', 'é’¢ç½‘æ¶å®‰è£…æ€»å·¥ç¨‹é‡', 'é’¢ç½‘æ¶æ‹†é™¤æ€»å·¥ç¨‹é‡',
        ]
        existing_relevant_quantity_columns = [col for col in all_relevant_quantity_columns if col in df.columns]

        for qty_col in existing_relevant_quantity_columns:
            rules[f'avg_qty_{qty_col}'] = df[qty_col].mean() if not df[qty_col].empty else 0.0
            self.logger.debug(f"å­¦ä¹ åˆ° '{qty_col}' çš„å¹³å‡æ•°é‡: {rules[f'avg_qty_{qty_col}']:.2f}")

        self.logger.debug(f"è®¡ç®— '{self.ratio_method_factor_col}' for {self.mode}ã€‚")
        if self.mode == 'steel_cage':
            core_costs_sum_temp = df['æˆæœ¬_å¡”åŠç§Ÿèµ'] + df['æˆæœ¬_é’¢ç­‹ç”Ÿäº§çº¿'] + df['æˆæœ¬_åŠç´¢å…·'] + df['æˆæœ¬_å¥—ç­’']
            self.logger.debug(f"æ ¸å¿ƒæˆæœ¬ä¸´æ—¶æ±‚å’Œç»Ÿè®¡ (steel_cage): {core_costs_sum_temp.describe()}")
            self.logger.debug(f"é¡¹ç›®æ€»ä»·ç»Ÿè®¡ (steel_cage): {df[self.target_column].describe()}")
            mask = (df[self.target_column] != 0) & (~np.isnan(df[self.target_column])) & (~np.isnan(core_costs_sum_temp))
            if mask.sum() > 0:
                calculated_ratio = (core_costs_sum_temp[mask] / df[self.target_column][mask]).mean()
                self.logger.debug(f"è®¡ç®—å‡ºçš„æ¯”ç‡å› å­ (steel_cage): {calculated_ratio}")
                rules[self.ratio_method_factor_col] = calculated_ratio
            else:
                self.logger.warning(f"æ— æ³•è®¡ç®—æœ‰æ•ˆæ¯”ç‡å› å­ (steel_cage)ï¼šåˆ†æ¯ä¸ºé›¶æˆ–æ•°æ®æ— æ•ˆã€‚")
                rules[self.ratio_method_factor_col] = 0.0
        elif self.mode == 'steel_lining':
            if self.ratio_method_factor_col in df.columns:
                self.logger.debug(f"é’¢è¡¬é‡Œæ¨¡å¼å·²åœ¨é¢„å¤„ç†ä¸­è®¡ç®— '{self.ratio_method_factor_col}'ã€‚å…¶ç»Ÿè®¡ä¿¡æ¯:\n{df[self.ratio_method_factor_col].describe()}")
            else:
                self.logger.error(f"é’¢è¡¬é‡Œæ¨¡å¼ '{self.ratio_method_factor_col}' åˆ—ä¸å­˜åœ¨ï¼")

        # å·¥ç¨‹é‡ä¹‹é—´çš„æ¯”ç‡
        if self.core_quantity_key in df.columns:
            quantities_to_learn_ratios = []
            if self.mode == 'steel_cage':
                quantities_to_learn_ratios.extend([
                    'å¡”åŠç§Ÿèµå·¥ç¨‹é‡', 'åŠç´¢å…·æ•°é‡', 'å¥—ç­’æ•°é‡'
                ])
                for feature in self.cluster_features_for_matching:
                    if feature != self.core_quantity_key:
                        quantities_to_learn_ratios.append(feature)
            elif self.mode == 'steel_lining':
                quantities_to_learn_ratios.extend([
                    'åˆ¶ä½œèƒå…·æ€»å·¥ç¨‹é‡', 'é’¢æ”¯å¢©åŸ‹ä»¶æ€»å·¥ç¨‹é‡', 'æ‰¶å£æŸ±æ€»å·¥ç¨‹é‡',
                    'èµ°é“æ¿æ“ä½œå¹³å°æ€»å·¥ç¨‹é‡', 'é’¢ç½‘æ¢æ€»å·¥ç¨‹é‡',
                ])
                for feature in self.cluster_features_for_matching:
                    if feature != self.core_quantity_key:
                        quantities_to_learn_ratios.append(feature)

            quantities_to_learn_ratios = list(set([q for q in quantities_to_learn_ratios if q in df.columns and q != self.core_quantity_key]))
            
            for qty_col in quantities_to_learn_ratios:
                valid_df = df[(df[self.core_quantity_key] > 0) & (df[qty_col].notna())].copy()
                if not valid_df.empty:
                    rules[f'avg_qty_ratio_{qty_col}_per_{self.core_quantity_key}'] = \
                        (pd.to_numeric(valid_df[qty_col], errors='coerce') / \
                         pd.to_numeric(valid_df[self.core_quantity_key], errors='coerce')).mean()
                    self.logger.debug(f"å­¦ä¹ åˆ° '{qty_col}' ç›¸å¯¹äº '{self.core_quantity_key}' çš„å¹³å‡æ¯”ç‡: {rules[f'avg_qty_ratio_{qty_col}_per_{self.core_quantity_key}']:.2f}")
                else:
                    rules[f'avg_qty_ratio_{qty_col}_per_{self.core_quantity_key}'] = 0.0
                    self.logger.warning(f"æ— æ³•å­¦ä¹  '{qty_col}' ç›¸å¯¹äº '{self.core_quantity_key}' çš„å¹³å‡æ¯”ç‡ï¼šæ ¸å¿ƒé‡ä¸ºé›¶æˆ–æ•°æ®æ— æ•ˆã€‚")

        # æ¯ä¸ªMLç‰¹å¾çš„å¹³å‡å•ä½æˆæœ¬
        for ml_feature, qty_col in self.ml_feature_to_qty_map.items():
            if ml_feature in df.columns and qty_col in df.columns:
                valid_df = df[(df[ml_feature] > 0) & (df[qty_col] > 0)].copy()
                if not valid_df.empty:
                    unit_cost = (pd.to_numeric(valid_df[ml_feature], errors='coerce') / 
                                 pd.to_numeric(valid_df[qty_col], errors='coerce')).median()
                    rules[f'avg_unit_cost_{ml_feature}'] = unit_cost
                    self.logger.debug(f"å­¦ä¹ åˆ° '{ml_feature}' çš„å¹³å‡å•ä½æˆæœ¬: {unit_cost:.2f} å…ƒ/{qty_col}")
                else:
                    rules[f'avg_unit_cost_{ml_feature}'] = 0.0
                    self.logger.warning(f"æ— æ³•å­¦ä¹  '{ml_feature}' çš„å¹³å‡å•ä½æˆæœ¬ï¼šè´¹ç”¨æˆ–å·¥ç¨‹é‡ä¸ºé›¶/æ— æ•ˆã€‚")
            else:
                rules[f'avg_unit_cost_{ml_feature}'] = 0.0
                self.logger.warning(f"æ— æ³•å­¦ä¹  '{ml_feature}' çš„å¹³å‡å•ä½æˆæœ¬ï¼šè´¹ç”¨åˆ—æˆ–å·¥ç¨‹é‡åˆ—ç¼ºå¤±ã€‚")

        return rules

    def _get_corresponding_quantity_column(self, ml_feature):
        """æ ¹æ®MLç‰¹å¾æ‰¾åˆ°å¯¹åº”çš„å·¥ç¨‹é‡åˆ—å"""
        mapping = {
            'æˆæœ¬_å¡”åŠç§Ÿèµ': 'å¡”åŠç§Ÿèµå·¥ç¨‹é‡',
            'æˆæœ¬_é’¢ç­‹ç”Ÿäº§çº¿': 'é’¢ç­‹æ€»å¨æ•°',
            'æˆæœ¬_åŠç´¢å…·': 'åŠç´¢å…·æ•°é‡',
            'æˆæœ¬_å¥—ç­’': 'å¥—ç­’æ•°é‡',

            'æ‹¼è£…åœºåœ°è´¹ç”¨': 'æ‹¼è£…åœºåœ°æ€»å·¥ç¨‹é‡',
            'åˆ¶ä½œèƒå…·è´¹ç”¨': 'åˆ¶ä½œèƒå…·æ€»å·¥ç¨‹é‡',
            'é’¢æ”¯å¢©ã€åŸ‹ä»¶è´¹ç”¨': 'é’¢æ”¯å¢©åŸ‹ä»¶æ€»å·¥ç¨‹é‡',
            'æ‰¶å£æŸ±è´¹ç”¨': 'æ‰¶å£æŸ±æ€»å·¥ç¨‹é‡',
            'èµ°é“æ¿åŠæ“ä½œå¹³å°è´¹ç”¨': 'èµ°é“æ¿æ“ä½œå¹³å°æ€»å·¥ç¨‹é‡',
            'é’¢ç½‘æ¶è´¹ç”¨': 'é’¢ç½‘æ¢æ€»å·¥ç¨‹é‡',
        }
        return mapping.get(ml_feature, None)

    def _train_ml_models(self):
        """
        æ”¹è¿›çš„æ¨¡å‹è®­ç»ƒæ–¹æ³• - ä½¿ç”¨æ•°æ®åº“ä¸­çš„å‚æ•°é…ç½®
        """
        
        # ã€æ–°å¢ã€‘ç®—æ³•å¯ç”¨æ€§æ£€æŸ¥
        if not self.load_algorithm_configs():
            self.logger.error("ç®—æ³•é…ç½®åŠ è½½å¤±è´¥ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            self.models = {}
            return
            
        availability = self.validate_algorithm_availability()
        if availability['status'] == 'unavailable':
            self.logger.warning("æ‰€æœ‰ç®—æ³•éƒ½å·²åœç”¨ï¼Œè·³è¿‡æ¨¡å‹è®­ç»ƒ")
            self.models = {}
            return
        
        # ã€æ–°å¢ã€‘åŠ è½½æ•°æ®åº“å‚æ•°
        if not self.parameters_loaded:
            if not self.load_algorithm_parameters_from_db():
                self.logger.warning(f"æ— æ³•åŠ è½½ç®—æ³•å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤é…ç½® for {self.mode}")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = self.df_historical[self.ml_features].copy()
        y = self.df_historical[self.target_column].copy()

        # å¤„ç†å¯èƒ½å­˜åœ¨çš„æ— ç©·å¤§å€¼
        X.replace([np.inf, -np.inf], np.nan, inplace=True)
        y.replace([np.inf, -np.inf], np.nan, inplace=True)

        # åˆå¹¶Xå’Œyï¼Œç„¶åä¸¢å¼ƒåŒ…å«NaNçš„è¡Œ
        combined_data = X.assign(target=y).dropna()
        if combined_data.empty:
            self.logger.warning(f"No valid data for ML model training after dropping NaNs for {self.mode}.")
            self.models = {}
            return

        X_cleaned = combined_data[self.ml_features]
        y_cleaned = combined_data['target']

        if X_cleaned.empty or len(X_cleaned) < 2:
            self.logger.warning(f"Not enough data to train ML models for {self.mode}. Skipping ML training.")
            self.models = {}
            return

        # æ£€æŸ¥ç‰¹å¾ä¸­æ˜¯å¦æœ‰æ‰€æœ‰å€¼éƒ½ä¸º0æˆ–NaNçš„åˆ—
        for col in X_cleaned.columns:
            if X_cleaned[col].nunique() == 1 and X_cleaned[col].iloc[0] == 0:
                self.logger.warning(f"Feature column '{col}' has only zero values. This might affect model training.")
            elif X_cleaned[col].isnull().all():
                self.logger.warning(f"Feature column '{col}' has all NaN values. Dropping this column for training.")
                X_cleaned = X_cleaned.drop(columns=[col])
        if X_cleaned.empty:
            self.logger.warning(f"All feature columns became empty after preprocessing for {self.mode}. Skipping ML training.")
            self.models = {}
            return

        # ã€ä¿®æ”¹ã€‘ä½¿ç”¨æ•°æ®åº“å‚æ•°é…ç½®åˆ›å»ºæ¨¡å‹
        models_config = self._create_models_with_db_params()

        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for model_name, pipeline in models_config.items():
            # æ£€æŸ¥ç®—æ³•æ˜¯å¦å¯ç”¨
            db_algorithm_name = get_db_algorithm_name(model_name)
            
            if db_algorithm_name and self.is_algorithm_enabled(db_algorithm_name):
                try:
                    pipeline.fit(X_cleaned, y_cleaned)
                    self.models[model_name] = pipeline
                    self.logger.info(f"  âœ… {model_name} è®­ç»ƒæˆåŠŸ (ä½¿ç”¨æ•°æ®åº“å‚æ•°é…ç½®)")
                except Exception as e:
                    self.logger.error(f"  âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
            else:
                self.logger.info(f"  ğŸš« {model_name} å·²åœç”¨ï¼Œè·³è¿‡è®­ç»ƒ")

        self.logger.info(f"æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆ for {self.mode}ï¼ŒæˆåŠŸè®­ç»ƒ {len(self.models)} ä¸ªæ¨¡å‹")

    def predict(self, user_inputs, unit_prices_from_db):
        """
        é¢„æµ‹é¡¹ç›®æ€»æˆæœ¬ - å®Œå…¨ç‹¬ç«‹å¹¶è¡Œçš„é¢„æµ‹æ–¹æ³•ç‰ˆæœ¬
        
        Parameters:
        - user_inputs: dict, ç”¨æˆ·è¾“å…¥å‚æ•°
        - unit_prices_from_db: dict, ä»ä»·æ ¼åŸºå‡†æ•°æ®åº“åŠ è½½çš„å•ä½ä»·æ ¼
        
        Returns:
        - dict: é¢„æµ‹ç»“æœï¼ˆå››ç§æ–¹æ³•å®Œå…¨ç‹¬ç«‹ï¼‰
        """
        if not self.is_trained:
            return {"error": f"ç³»ç»Ÿå°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train_system() for {self.mode}"}

        try:
            # 1. ä¼°ç®—æ‰€æœ‰å·¥ç¨‹é‡å‚æ•°ï¼ˆè¿™éƒ¨åˆ†ä¿æŒä¸å˜ï¼‰
            main_quantity_input = user_inputs.get(self.core_quantity_key)
            if main_quantity_input is None:
                for key, val in user_inputs.items():
                    if val is not None and val > 0 and key in self.cluster_features_for_matching:
                        main_quantity_input = val
                        self.logger.info(f"Using provided '{key}' ({main_quantity_input}) as main quantity for estimation.")
                        break
            if main_quantity_input is None or main_quantity_input <= 0:
                self.logger.error(f"æ— æ³•ä»è¾“å…¥ä¸­è·å–æœ‰æ•ˆçš„æ ¸å¿ƒå·¥ç¨‹é‡ ({self.core_quantity_key}) æˆ–å…¶ä»–èšç±»ç‰¹å¾æ¥ä¼°ç®—å…¶ä»–å·¥ç¨‹é‡ for {self.mode}.")
                main_quantity_input = 0

            # åŒ¹é…é›†ç¾¤è§„åˆ™
            selected_rules = self._match_cluster_rules(main_quantity_input)

            # æ ¹æ®è§„åˆ™ä¼°ç®—æ‰€æœ‰å·¥ç¨‹é‡å‚æ•°
            estimated_quantities = self._estimate_quantities_based_on_rules(user_inputs, main_quantity_input, selected_rules)
            if estimated_quantities is None:
                return {"error": f"æ— æ³•ä»è¾“å…¥å‚æ•°ä¼°ç®—æ‰€æœ‰å·¥ç¨‹é‡ for {self.mode}ï¼Œè¯·è‡³å°‘æä¾›ä¸€ä¸ªæœ‰æ•ˆå‚æ•°ã€‚"}

            # 2. è®¡ç®—å„é¡¹æˆæœ¬ (å¯¹åº” ML Features)
            calculated_ml_features = self._calculate_ml_features_from_quantities_and_unit_prices(
                estimated_quantities, unit_prices_from_db
            )
            # ç¡®ä¿æ‰€æœ‰ ML_FEATURES éƒ½å­˜åœ¨ä¸”ä¸ºæ•°å€¼
            for feature in self.ml_features:
                if feature not in calculated_ml_features or calculated_ml_features[feature] is None:
                    calculated_ml_features[feature] = 0.0

            self.logger.info(f"Calculated ML features for {self.mode}: {calculated_ml_features}")

            # 3. è·å–æªæ–½è´¹
            measures_cost_value = user_inputs.get('æªæ–½è´¹å·¥ç¨‹é‡', 0.0)

            # 4. å››ç§ç‹¬ç«‹çš„é¢„æµ‹æ–¹æ³• - é‡æ„åçš„é€»è¾‘
            prediction_results = {}
            
            # === ç¬¬ä¸€æ­¥ï¼šåŸºäºç®—æ³•èƒ½åŠ›æ‰§è¡Œé¢„æµ‹è®¡ç®— ===
            ai_raw_result = None
            ai_final_result = None
            ratio_raw_result = None
            ratio_final_result = None
            
            # AIé¢„æµ‹è®¡ç®—ï¼ˆåŸºäºç®—æ³•å¯ç”¨æ€§ï¼‰
            if self.can_execute_ai_prediction():
                try:
                    ai_raw_result = self._ml_predict(calculated_ml_features)
                    ai_final_result = self._add_measures_cost_to_predictions(
                        ai_raw_result.copy() if ai_raw_result else None, 
                        measures_cost_value
                    )
                    self.logger.info(f"âœ… AIé¢„æµ‹è®¡ç®—å®Œæˆ for {self.mode}")
                except Exception as e:
                    self.logger.error(f"âŒ AIé¢„æµ‹è®¡ç®—å¤±è´¥ for {self.mode}: {e}")
                    ai_raw_result = {"error": f"AIé¢„æµ‹è®¡ç®—å¼‚å¸¸: {str(e)}"}
                    ai_final_result = {"error": f"AIé¢„æµ‹è®¡ç®—å¼‚å¸¸: {str(e)}"}
            else:
                algo_status = self.check_algorithm_execution_capability()
                ai_error_msg = {
                    "error": "AIé¢„æµ‹æ— æ³•æ‰§è¡Œ", 
                    "reason": algo_status['message'],
                    "details": f"å¯ç”¨ç®—æ³•: {algo_status['enabled_count']}/{algo_status['total_count']}"
                }
                ai_raw_result = ai_error_msg.copy()
                ai_final_result = ai_error_msg.copy()
                self.logger.warning(f"âš ï¸ AIé¢„æµ‹è·³è¿‡æ‰§è¡Œ for {self.mode}: {algo_status['message']}")

            # æ¯”ç‡æ³•é¢„æµ‹è®¡ç®—ï¼ˆåŸºäºæ¯”ç‡æ³•å¯ç”¨æ€§ï¼‰
            if self.can_execute_ratio_prediction():
                try:
                    ratio_raw_result = self._ratio_method_predict(calculated_ml_features, selected_rules)
                    if isinstance(ratio_raw_result, (int, float)):
                        ratio_final_result = ratio_raw_result + measures_cost_value
                    else:
                        ratio_final_result = ratio_raw_result
                    self.logger.info(f"âœ… æ¯”ç‡æ³•é¢„æµ‹è®¡ç®—å®Œæˆ for {self.mode}")
                except Exception as e:
                    self.logger.error(f"âŒ æ¯”ç‡æ³•é¢„æµ‹è®¡ç®—å¤±è´¥ for {self.mode}: {e}")
                    ratio_raw_result = {"error": f"æ¯”ç‡æ³•é¢„æµ‹è®¡ç®—å¼‚å¸¸: {str(e)}"}
                    ratio_final_result = {"error": f"æ¯”ç‡æ³•é¢„æµ‹è®¡ç®—å¼‚å¸¸: {str(e)}"}
            else:
                ratio_error_msg = {
                    "error": "æ¯”ç‡æ³•é¢„æµ‹æ— æ³•æ‰§è¡Œ",
                    "reason": "ç¼ºå°‘å¿…è¦çš„å†å²æ•°æ®æˆ–è§„åˆ™åº“",
                    "details": f"è®­ç»ƒçŠ¶æ€: {self.is_trained}, å†å²æ•°æ®: {len(self.df_historical) if self.df_historical is not None else 0}æ¡"
                }
                ratio_raw_result = ratio_error_msg.copy()
                ratio_final_result = ratio_error_msg.copy()
                self.logger.warning(f"âš ï¸ æ¯”ç‡æ³•é¢„æµ‹è·³è¿‡æ‰§è¡Œ for {self.mode}: ç¼ºå°‘å¿…è¦æ•°æ®")

            # === ç¬¬äºŒæ­¥ï¼šåŸºäºæ˜¾ç¤ºæƒé™å†³å®šæœ€ç»ˆè¾“å‡º ===
            prediction_methods = [
                ('ml_prediction_raw', 'AIé¢„æµ‹-åŸå§‹å€¼', ai_raw_result),
                ('ml_prediction_final', 'AIé¢„æµ‹-æœ€ç»ˆå€¼', ai_final_result),
                ('ratio_method_raw', 'æ¯”ç‡æ³•-åŸå§‹å€¼', ratio_raw_result),
                ('ratio_method_final', 'æ¯”ç‡æ³•-æœ€ç»ˆå€¼', ratio_final_result)
            ]

            for method_key, display_name, computed_result in prediction_methods:
                combined_status = self.get_combined_prediction_status(method_key)
                
                if combined_status['final_status'] == 'fully_available':
                    # å¯ä»¥æ‰§è¡Œä¸”å¯ä»¥æ˜¾ç¤º
                    prediction_results[display_name] = computed_result
                    
                elif combined_status['final_status'] == 'execute_only':
                    # å¯ä»¥æ‰§è¡Œä½†æ˜¾ç¤ºè¢«ç¦ç”¨
                    prediction_results[display_name] = {
                        "status": "display_disabled",
                        "message": f"ğŸš« {combined_status['display_status']['message']}",
                        "indicator_code": combined_status['display_status']['indicator_code'],
                        "hidden_result_available": True  # æ ‡è®°å®é™…æœ‰è®¡ç®—ç»“æœä½†è¢«éšè—
                    }
                    
                elif combined_status['final_status'] == 'display_error':
                    # æƒ³æ˜¾ç¤ºä½†æ— æ³•æ‰§è¡Œ
                    prediction_results[display_name] = {
                        "status": "execution_failed",
                        "message": f"âŒ æ— æ³•æ‰§è¡Œé¢„æµ‹è®¡ç®—",
                        "reason": combined_status['execution_status']['message'],
                        "suggestion": f"è‹¥éœ€ä½¿ç”¨{display_name}ï¼Œè¯·åˆ°æ•°æ®ç®¡ç†æ¨¡å—å¯ç”¨ç›¸å…³ç®—æ³•"
                    }
                    
                else:  # fully_disabled
                    # æ—¢ä¸èƒ½æ‰§è¡Œä¹Ÿä¸èƒ½æ˜¾ç¤º
                    prediction_results[display_name] = {
                        "status": "fully_disabled", 
                        "message": f"ğŸš« {display_name}å·²å®Œå…¨ç¦ç”¨",
                        "execution_reason": combined_status['execution_status']['message'],
                        "display_reason": combined_status['display_status']['message']
                    }

                # æ·»åŠ è¯¦ç»†çš„çŠ¶æ€æ—¥å¿—
                self.logger.info(f"ğŸ“Š {display_name} æœ€ç»ˆçŠ¶æ€: {combined_status['final_status']}")

            # 5. æ·»åŠ è¯¦ç»†çš„çŠ¶æ€æ§åˆ¶ä¿¡æ¯åˆ°ç»“æœä¸­
            prediction_results["é¢„æµ‹æ–¹æ³•çŠ¶æ€"] = {}
            prediction_results["ç®—æ³•æ‰§è¡ŒçŠ¶æ€"] = self.check_algorithm_execution_capability()
            prediction_results["æ˜¾ç¤ºæƒé™çŠ¶æ€"] = {}
            
            for method_key, display_name, _ in prediction_methods:
                combined_status = self.get_combined_prediction_status(method_key)
                prediction_results["é¢„æµ‹æ–¹æ³•çŠ¶æ€"][method_key] = {
                    'name': display_name,
                    'can_execute': combined_status['can_execute'],
                    'can_display': combined_status['can_display'],
                    'final_status': combined_status['final_status'],
                    'execution_message': combined_status['execution_status']['message'],
                    'display_message': combined_status['display_status']['message']
                }
                prediction_results["æ˜¾ç¤ºæƒé™çŠ¶æ€"][method_key] = combined_status['display_status']

# 6. ç”ŸæˆçŠ¶æ€æ±‡æ€»ä¿¡æ¯
            prediction_results["çŠ¶æ€æ±‡æ€»"] = get_prediction_status_summary(self.mode)

            # 7. ä¸ºäº†å…¼å®¹ç°æœ‰çš„ç•Œé¢ä»£ç ï¼Œæ™ºèƒ½é€‰æ‹©ä¸»è¦æ˜¾ç¤ºç»“æœ
            # ä¼˜å…ˆé€‰æ‹©å¯ä»¥æ­£å¸¸æ‰§è¡Œä¸”å¯ä»¥æ˜¾ç¤ºçš„æ–¹æ³•
            main_ml_result = None
            main_ratio_result = None
            
            # AIé¢„æµ‹ç»“æœé€‰æ‹©é€»è¾‘ï¼šä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆç»“æœ
            ai_methods = [
                ('AIé¢„æµ‹-åŸå§‹å€¼', ai_raw_result),
                ('AIé¢„æµ‹-æœ€ç»ˆå€¼', ai_final_result)
            ]
            
            for method_name, result in ai_methods:
                method_key = 'ml_prediction_raw' if 'åŸå§‹å€¼' in method_name else 'ml_prediction_final'
                combined_status = self.get_combined_prediction_status(method_key)
                
                if combined_status['final_status'] == 'fully_available':
                    # ä¼˜å…ˆé€‰æ‹©å®Œå…¨å¯ç”¨çš„æ–¹æ³•
                    if result and not isinstance(result, dict) or (isinstance(result, dict) and 'error' not in result):
                        main_ml_result = result
                        self.logger.info(f"é€‰æ‹© {method_name} ä½œä¸ºä¸»è¦AIé¢„æµ‹ç»“æœ")
                        break
                elif combined_status['can_execute'] and result:
                    # å¦‚æœæ²¡æœ‰å®Œå…¨å¯ç”¨çš„ï¼Œé€‰æ‹©å¯æ‰§è¡Œçš„ä½œä¸ºå¤‡é€‰
                    if main_ml_result is None:
                        main_ml_result = result
                        self.logger.info(f"å¤‡é€‰: {method_name} ä½œä¸ºä¸»è¦AIé¢„æµ‹ç»“æœ")

            # æ¯”ç‡æ³•é¢„æµ‹ç»“æœé€‰æ‹©é€»è¾‘ï¼šåŒæ ·ä¼˜å…ˆé€‰æ‹©æœ‰æ•ˆç»“æœ
            ratio_methods = [
                ('æ¯”ç‡æ³•-åŸå§‹å€¼', ratio_raw_result),
                ('æ¯”ç‡æ³•-æœ€ç»ˆå€¼', ratio_final_result)
            ]
            
            for method_name, result in ratio_methods:
                method_key = 'ratio_method_raw' if 'åŸå§‹å€¼' in method_name else 'ratio_method_final'
                combined_status = self.get_combined_prediction_status(method_key)
                
                if combined_status['final_status'] == 'fully_available':
                    # ä¼˜å…ˆé€‰æ‹©å®Œå…¨å¯ç”¨çš„æ–¹æ³•
                    if result and not isinstance(result, dict) or (isinstance(result, dict) and 'error' not in result):
                        main_ratio_result = result
                        self.logger.info(f"é€‰æ‹© {method_name} ä½œä¸ºä¸»è¦æ¯”ç‡æ³•é¢„æµ‹ç»“æœ")
                        break
                elif combined_status['can_execute'] and result:
                    # å¦‚æœæ²¡æœ‰å®Œå…¨å¯ç”¨çš„ï¼Œé€‰æ‹©å¯æ‰§è¡Œçš„ä½œä¸ºå¤‡é€‰
                    if main_ratio_result is None:
                        main_ratio_result = result
                        self.logger.info(f"å¤‡é€‰: {method_name} ä½œä¸ºä¸»è¦æ¯”ç‡æ³•é¢„æµ‹ç»“æœ")

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ä¸»è¦ç»“æœï¼Œä½¿ç”¨é”™è¯¯ä¿¡æ¯
            if main_ml_result is None:
                ai_capability = self.check_algorithm_execution_capability()
                main_ml_result = {
                    "error": "AIé¢„æµ‹ä¸å¯ç”¨",
                    "reason": ai_capability['message'],
                    "suggestion": "è¯·æ£€æŸ¥ç®—æ³•é…ç½®æˆ–ç»¼åˆæŒ‡æ ‡è®¾ç½®"
                }
                self.logger.warning(f"æœªæ‰¾åˆ°å¯ç”¨çš„AIé¢„æµ‹ç»“æœï¼Œä½¿ç”¨é”™è¯¯ä¿¡æ¯")

            if main_ratio_result is None:
                main_ratio_result = {
                    "error": "æ¯”ç‡æ³•é¢„æµ‹ä¸å¯ç”¨",
                    "reason": "ç¼ºå°‘å¿…è¦çš„å†å²æ•°æ®æˆ–æ˜¾ç¤ºæƒé™",
                    "suggestion": "è¯·æ£€æŸ¥æ•°æ®é…ç½®æˆ–ç»¼åˆæŒ‡æ ‡è®¾ç½®"
                }
                self.logger.warning(f"æœªæ‰¾åˆ°å¯ç”¨çš„æ¯”ç‡æ³•é¢„æµ‹ç»“æœï¼Œä½¿ç”¨é”™è¯¯ä¿¡æ¯")

            # æ•´åˆç»“æœï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            results = {
                "åŒ¹é…åˆ°çš„è§„åˆ™æ¥æº": selected_rules.get('source', 'å…¨å±€å¹³å‡'),
                "ä¼°ç®—çš„å·¥ç¨‹é‡": estimated_quantities,
                "ä¼°ç®—çš„å„é¡¹æˆæœ¬ (ç”¨äºMLçš„ç‰¹å¾)": calculated_ml_features,
                # ä¸»è¦ç»“æœï¼ˆç”¨äºå…¼å®¹ç°æœ‰ç•Œé¢ï¼‰- ä¿®æ”¹åçš„æ™ºèƒ½é€‰æ‹©é€»è¾‘
                "æœºå™¨å­¦ä¹ é¢„æµ‹ç»“æœ": main_ml_result,
                "æ¯”ç‡æ³•é¢„æµ‹æ€»ä»·": main_ratio_result,
                # æ–°å¢ï¼šå››ç§ç‹¬ç«‹çš„é¢„æµ‹ç»“æœ
                "AIé¢„æµ‹-åŸå§‹å€¼": prediction_results["AIé¢„æµ‹-åŸå§‹å€¼"],
                "AIé¢„æµ‹-æœ€ç»ˆå€¼": prediction_results["AIé¢„æµ‹-æœ€ç»ˆå€¼"],
                "æ¯”ç‡æ³•-åŸå§‹å€¼": prediction_results["æ¯”ç‡æ³•-åŸå§‹å€¼"],
                "æ¯”ç‡æ³•-æœ€ç»ˆå€¼": prediction_results["æ¯”ç‡æ³•-æœ€ç»ˆå€¼"],
                # çŠ¶æ€ä¿¡æ¯ - ä¿®æ”¹åçš„è¯¦ç»†çŠ¶æ€
                "é¢„æµ‹æ–¹æ³•çŠ¶æ€": prediction_results["é¢„æµ‹æ–¹æ³•çŠ¶æ€"],
                "ç®—æ³•æ‰§è¡ŒçŠ¶æ€": prediction_results["ç®—æ³•æ‰§è¡ŒçŠ¶æ€"],
                "æ˜¾ç¤ºæƒé™çŠ¶æ€": prediction_results["æ˜¾ç¤ºæƒé™çŠ¶æ€"],
                "çŠ¶æ€æ±‡æ€»": prediction_results["çŠ¶æ€æ±‡æ€»"],
                "ç®—æ³•å¯ç”¨æ€§çŠ¶æ€": self.validate_algorithm_availability(),
                "ç”¨æˆ·è¾“å…¥": user_inputs,
                "æªæ–½è´¹": measures_cost_value
            }
            # ã€æ–°å¢ã€‘æ·»åŠ æœ€ä½³ç®—æ³•å‚æ•°ä¿¡æ¯
            try:
                # è·å–ç”¨äºæ¯”è¾ƒçš„é¢„æµ‹ç»“æœ
                comparison_ml_result = main_ml_result
                comparison_ratio_result = main_ratio_result
                
                # å¦‚æœä¸»è¦ç»“æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆé”™è¯¯æˆ–çŠ¶æ€ä¿¡æ¯ï¼‰ï¼Œå°è¯•ä»åŸå§‹é¢„æµ‹ç»“æœä¸­è·å–
                if isinstance(comparison_ml_result, dict) and 'error' in comparison_ml_result:
                    # å°è¯•ä»AIé¢„æµ‹åŸå§‹å€¼æˆ–æœ€ç»ˆå€¼ä¸­è·å–æ•°å€¼ç»“æœ
                    ai_raw = prediction_results.get("AIé¢„æµ‹-åŸå§‹å€¼")
                    ai_final = prediction_results.get("AIé¢„æµ‹-æœ€ç»ˆå€¼")
                    
                    if isinstance(ai_raw, dict) and 'error' not in ai_raw:
                        comparison_ml_result = ai_raw
                    elif isinstance(ai_final, dict) and 'error' not in ai_final:
                        comparison_ml_result = ai_final
                
                if isinstance(comparison_ratio_result, dict) and 'error' in comparison_ratio_result:
                    # å°è¯•ä»æ¯”ç‡æ³•åŸå§‹å€¼æˆ–æœ€ç»ˆå€¼ä¸­è·å–æ•°å€¼ç»“æœ
                    ratio_raw = prediction_results.get("æ¯”ç‡æ³•-åŸå§‹å€¼")
                    ratio_final = prediction_results.get("æ¯”ç‡æ³•-æœ€ç»ˆå€¼")
                    
                    if isinstance(ratio_raw, (int, float)):
                        comparison_ratio_result = ratio_raw
                    elif isinstance(ratio_final, (int, float)):
                        comparison_ratio_result = ratio_final
                
                # è·å–æœ€ä½³ç®—æ³•ä¿¡æ¯
                if (isinstance(comparison_ml_result, dict) and 
                    isinstance(comparison_ratio_result, (int, float))):
                    # å¦‚æœMLç»“æœæ˜¯å­—å…¸ï¼ˆåŒ…å«å¤šä¸ªç®—æ³•ç»“æœï¼‰
                    best_algorithm_info = self.get_best_algorithm_info_with_params(
                        comparison_ml_result, comparison_ratio_result
                    )
                    results["æœ€ä½³ç®—æ³•ä¿¡æ¯"] = best_algorithm_info
                    self.logger.info(f"âœ… å·²æ·»åŠ æœ€ä½³ç®—æ³•ä¿¡æ¯: {best_algorithm_info.get('best_algorithm_name', 'æœªç¡®å®š')}")
                else:
                    # å¦‚æœæ— æ³•è·å–æœ‰æ•ˆçš„æ¯”è¾ƒæ•°æ®
                    results["æœ€ä½³ç®—æ³•ä¿¡æ¯"] = {
                        "best_algorithm_name": None,
                        "selection_reason": "æ— æ³•è·å–æœ‰æ•ˆçš„é¢„æµ‹ç»“æœè¿›è¡Œæ¯”è¾ƒ"
                    }
                    self.logger.warning("âš ï¸ æ— æ³•ç¡®å®šæœ€ä½³ç®—æ³•ï¼šç¼ºå°‘æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
                    
            except Exception as e:
                self.logger.error(f"âŒ æ·»åŠ æœ€ä½³ç®—æ³•ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                results["æœ€ä½³ç®—æ³•ä¿¡æ¯"] = {
                    "best_algorithm_name": None,
                    "selection_reason": f"è·å–æœ€ä½³ç®—æ³•ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"
                }
            return results

        except Exception as e:
            self.logger.error(f"é¢„æµ‹è¿‡ç¨‹å‡ºé”™ for {self.mode}: {e}", exc_info=True)
            return {"error": f"é¢„æµ‹è¿‡ç¨‹å¼‚å¸¸ for {self.mode}: {e}"}

    def _add_measures_cost_to_predictions(self, ml_predictions, measures_cost_value):
        """
        ä¸ºæœºå™¨å­¦ä¹ é¢„æµ‹ç»“æœæ·»åŠ æªæ–½è´¹çš„è¾…åŠ©æ–¹æ³•ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
        
        Args:
            ml_predictions (dict): MLé¢„æµ‹ç»“æœ
            measures_cost_value (float): æªæ–½è´¹æ•°å€¼
            
        Returns:
            dict: æ›´æ–°åçš„é¢„æµ‹ç»“æœ
        """
        if not isinstance(ml_predictions, dict) or 'status' in ml_predictions:
            return ml_predictions
        
        result = ml_predictions.copy()
        for model_key, prediction_value in result.items():
            if isinstance(prediction_value, (int, float)) and prediction_value is not None:
                result[model_key] += measures_cost_value
                self.logger.debug(f"ä¸º {self.mode} æ¨¡å¼çš„ {model_key} æ·»åŠ æªæ–½è´¹ {measures_cost_value}")
        
        return result


    def _match_cluster_rules(self, main_quantity_input):
        """åŒ¹é…æœ€é€‚åˆçš„èšç±»è§„åˆ™"""
        if self.use_clustering and self.kmeans and self.scaler_cluster and main_quantity_input is not None and main_quantity_input > 0:
            cluster_input_data = {}
            for feature in self.cluster_features_for_matching:
                if feature == self.core_quantity_key:
                    cluster_input_data[feature] = main_quantity_input
                else:
                    ratio_key = f'avg_qty_ratio_{feature}_per_{self.core_quantity_key}'
                    if ratio_key in self.global_rules and self.global_rules[ratio_key] is not None:
                        cluster_input_data[feature] = main_quantity_input * self.global_rules[ratio_key]
                    else:
                        cluster_input_data[feature] = 0

            current_project_features_df = pd.DataFrame([cluster_input_data])
            current_project_features_df = current_project_features_df[[f for f in self.cluster_features_for_matching if f in current_project_features_df.columns]]

            try:
                current_project_scaled = self.scaler_cluster.transform(current_project_features_df)
                predicted_cluster = self.kmeans.predict(current_project_scaled)[0]
                selected_rules = self.cluster_rules.get(predicted_cluster, self.global_rules)
                selected_rules['source'] = f"ç°‡ {predicted_cluster}"
                self.logger.info(f"Matched to cluster {predicted_cluster} for {self.mode}.")
            except Exception as e:
                self.logger.warning(f"èšç±»åŒ¹é…å¤±è´¥ for {self.mode}: {e}. ä½¿ç”¨å…¨å±€å¹³å‡è§„åˆ™ã€‚", exc_info=True)
                selected_rules = self.global_rules.copy()
                selected_rules['source'] = "å…¨å±€å¹³å‡ (åŒ¹é…å¤±è´¥)"
        else:
            selected_rules = self.global_rules.copy()
            selected_rules['source'] = "å…¨å±€å¹³å‡"
            if not self.use_clustering:
                self.logger.info(f"Clustering disabled or not enough data for {self.mode}. Using global rules.")
            elif not self.kmeans:
                self.logger.warning(f"KMeans model not trained for {self.mode}. Using global rules.")

        return selected_rules

    def _estimate_quantities_based_on_rules(self, user_inputs, main_quantity_input, rules):
        estimated_quantities = {}

        all_relevant_quantity_columns = [
            'æ‹¼è£…åœºåœ°æ€»å·¥ç¨‹é‡', 'åˆ¶ä½œèƒå…·æ€»å·¥ç¨‹é‡', 'é’¢æ”¯å¢©åŸ‹ä»¶æ€»å·¥ç¨‹é‡', 'æ‰¶å£æŸ±æ€»å·¥ç¨‹é‡',
            'èµ°é“æ¿æ“ä½œå¹³å°æ€»å·¥ç¨‹é‡', 'é’¢ç½‘æ¢æ€»å·¥ç¨‹é‡',
            'é’¢ç­‹æ€»å¨æ•°', 'å¡”åŠç§Ÿèµå·¥ç¨‹é‡', 'åŠç´¢å…·æ•°é‡', 'å¥—ç­’æ•°é‡',
            'é’¢æ”¯å¢©åŸ‹ä»¶æ··å‡åœŸå‰”å‡¿æ€»å·¥ç¨‹é‡', 'é’¢æ”¯å¢©åŸ‹ä»¶æ··å‡åœŸå›å¡«æ€»å·¥ç¨‹é‡',
            'é’¢æ”¯å¢©åŸ‹ä»¶å®‰è£…æ€»å·¥ç¨‹é‡', 'é’¢æ”¯å¢©åŸ‹ä»¶åˆ¶ä½œæ€»å·¥ç¨‹é‡',
            'æ‰¶å£æŸ±å®‰è£…æ€»å·¥ç¨‹é‡', 'æ‰¶å£æŸ±æ‹†é™¤æ€»å·¥ç¨‹é‡', 'æ‰¶å£æŸ±æ„ä»¶ä½¿ç”¨æŠ˜æ—§æ€»å·¥ç¨‹é‡',
            'èµ°é“æ¿æ“ä½œå¹³å°åˆ¶ä½œæ€»å·¥ç¨‹é‡', 'èµ°é“æ¿æ“ä½œå¹³å°æ­è®¾æ€»å·¥ç¨‹é‡', 'èµ°é“æ¿æ“ä½œå¹³å°æ‹†é™¤æ€»å·¥ç¨‹é‡',
            'é’¢ç½‘æ¶åˆ¶ä½œæ€»å·¥ç¨‹é‡', 'é’¢ç½‘æ¶å®‰è£…æ€»å·¥ç¨‹é‡', 'é’¢ç½‘æ¶æ‹†é™¤æ€»å·¥ç¨‹é‡',
        ]
        relevant_quantity_columns = [col for col in all_relevant_quantity_columns if col in self.df_historical.columns]

        for qty_col in relevant_quantity_columns:
            user_val = user_inputs.get(qty_col)
            if user_val is not None and pd.notna(user_val) and float(user_val) > 0:
                estimated_quantities[qty_col] = float(user_val)
                self.logger.debug(f"Using user provided value for {qty_col}: {float(user_val)}")
            elif qty_col == self.core_quantity_key and (main_quantity_input is not None and main_quantity_input > 0):
                estimated_quantities[qty_col] = main_quantity_input
                self.logger.debug(f"Using main_quantity_input for {qty_col}: {main_quantity_input}")
            elif main_quantity_input is not None and main_quantity_input > 0:
                ratio_key = f'avg_qty_ratio_{qty_col}_per_{self.core_quantity_key}'
                if ratio_key in rules and rules[ratio_key] is not None:
                    estimated_value = main_quantity_input * rules[ratio_key]
                    estimated_quantities[qty_col] = estimated_value
                    self.logger.debug(f"Estimated {qty_col} using ratio: {estimated_value}")
                else:
                    avg_qty_key = f'avg_qty_{qty_col}'
                    if avg_qty_key in rules and rules[avg_qty_key] is not None:
                        estimated_quantities[qty_col] = rules[avg_qty_key]
                        self.logger.debug(f"Estimated {qty_col} using average quantity from rules: {rules[avg_qty_key]}")
                    else:
                        estimated_quantities[qty_col] = 0.0
                        self.logger.warning(f"Cannot estimate {qty_col} for {self.mode}. No ratio or average found.")
            else:
                avg_qty_key = f'avg_qty_{qty_col}'
                if avg_qty_key in rules and rules[avg_qty_key] is not None:
                    estimated_quantities[qty_col] = rules[avg_qty_key]
                    self.logger.debug(f"Estimated {qty_col} using average quantity from rules (no main_quantity_input): {rules[avg_qty_key]}")
                else:
                    estimated_quantities[qty_col] = 0.0
                    self.logger.warning(f"Cannot estimate {qty_col} for {self.mode}. No main_quantity_input, ratio, or average found.")

        return estimated_quantities

    def _calculate_ml_features_from_quantities_and_unit_prices(self, estimated_quantities, unit_prices_from_db):
        """æ ¹æ®ä¼°ç®—çš„å·¥ç¨‹é‡å’Œå†å²å­¦ä¹ åˆ°çš„å¹³å‡å•ä½æˆæœ¬è®¡ç®—æœºå™¨å­¦ä¹ æ¨¡å‹æ‰€éœ€çš„ç‰¹å¾"""
        calculated_features = {}
        selected_rules = self._match_cluster_rules(estimated_quantities.get(self.core_quantity_key, 0))

        for ml_feature, qty_col in self.ml_feature_to_qty_map.items():
            qty = estimated_quantities.get(qty_col, 0.0)
            avg_unit_cost_key = f'avg_unit_cost_{ml_feature}'
            avg_unit_cost = selected_rules.get(avg_unit_cost_key, 0.0)

            calculated_features[ml_feature] = qty * avg_unit_cost
            self.logger.debug(f"è®¡ç®—é¢„æµ‹MLç‰¹å¾ '{ml_feature}': å·¥ç¨‹é‡ {qty:.2f} * å¹³å‡å•ä½æˆæœ¬ {avg_unit_cost:.2f} = {calculated_features[ml_feature]:.2f}")

        return calculated_features

    def _ml_predict(self, calculated_ml_features):
        """ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹ - æ”¹è¿›ç‰ˆæœ¬"""
        feature_vector_dict = {col: [calculated_ml_features.get(col, 0.0)] for col in self.ml_features}
        feature_df = pd.DataFrame(feature_vector_dict)
        
        for col in self.ml_features:
            if col not in feature_df.columns:
                feature_df[col] = 0.0
        feature_df = feature_df[self.ml_features]

        predictions = {}
        
        # é¦–å…ˆå¤„ç†å¯ç”¨çš„ç®—æ³•
        for model_name, model in self.models.items():
            try:
                if isinstance(model, Pipeline) and isinstance(model.steps[0][1], StandardScaler):
                    scaled_features = model.named_steps['scaler'].transform(feature_df)
                    pred = model.named_steps['regressor'].predict(scaled_features)[0]
                else:
                    pred = model.predict(feature_df)[0]

                if 0 < pred < 5000000000:
                    predictions[model_name] = pred
                    self.logger.debug(f"âœ… {model_name} é¢„æµ‹æˆåŠŸ: {pred:.2f}")
                else:
                    predictions[model_name] = None
                    self.logger.warning(f"âš ï¸ {model_name} é¢„æµ‹å€¼è¶…å‡ºåˆç†èŒƒå›´: {pred:.2f}")
            except Exception as e:
                predictions[model_name] = None
                self.logger.error(f"âŒ {model_name} é¢„æµ‹å¤±è´¥: {e}")

        # ç„¶åä¸ºåœç”¨çš„ç®—æ³•æ·»åŠ çŠ¶æ€ä¿¡æ¯
        algorithm_status = self.get_algorithm_status_info()
        for disabled_algorithm in algorithm_status['disabled']:
            model_key = disabled_algorithm['display_name']
            predictions[model_key] = {
                'status': 'disabled',
                'message': "ç®—æ³•å·²åœç”¨ - è‹¥éœ€å¯ç”¨è¯·åˆ°æ•°æ®ç®¡ç†æ¨¡å—",
                'db_name': disabled_algorithm['db_name']
            }
            self.logger.debug(f"ğŸš« {model_key} å·²åœç”¨ï¼Œæ·»åŠ çŠ¶æ€ä¿¡æ¯")

        # è®¡ç®—é›†æˆé¢„æµ‹ï¼ˆåªåŸºäºå¯ç”¨ä¸”æˆåŠŸçš„ç®—æ³•ï¼‰
        valid_predictions = [
            p for p in predictions.values() 
            if isinstance(p, (int, float)) and p is not None
        ]
        
        if valid_predictions:
            ensemble_avg = np.mean(valid_predictions)
            predictions['é›†æˆå¹³å‡é¢„æµ‹'] = ensemble_avg
            self.logger.info(f"ğŸ¯ é›†æˆé¢„æµ‹åŸºäº {len(valid_predictions)} ä¸ªæœ‰æ•ˆç®—æ³•: {ensemble_avg:.2f}")
        else:
            predictions['é›†æˆå¹³å‡é¢„æµ‹'] = None
            self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„ç®—æ³•é¢„æµ‹ç»“æœï¼Œæ— æ³•è®¡ç®—é›†æˆé¢„æµ‹")
        
        return predictions

    def _ratio_method_predict(self, calculated_ml_features, rules):
        """ä½¿ç”¨æ¯”ç‡æ³•é¢„æµ‹æ€»ä»·"""
        core_costs_sum = sum(calculated_ml_features.get(col, 0) for col in self.ml_features)
        print(core_costs_sum)
        
        avg_percentage = rules.get(self.ratio_method_factor_col)
        print(avg_percentage)
        if avg_percentage is None or np.isnan(avg_percentage) or avg_percentage <= 0:
            self.logger.warning(f"æ¯”ç‡æ³•å› å­ '{self.ratio_method_factor_col}' æ— æ•ˆæˆ–ä¸ºé›¶ for {self.mode}. ä½¿ç”¨é»˜è®¤æ¯”ä¾‹ 0.8ã€‚")
            avg_percentage = 0.8

        if core_costs_sum > 0 and avg_percentage > 0:
            return core_costs_sum / avg_percentage
        else:
            self.logger.warning(f"æ¯”ç‡æ³•é¢„æµ‹æ¡ä»¶ä¸è¶³ (æ ¸å¿ƒæˆæœ¬ä¸º0æˆ–å¹³å‡æ¯”ä¾‹ä¸º0) for {self.mode}.")
            return 0.0
    # ================== CostPredictionSystem ç±»æ–¹æ³•ï¼ˆæ”¾åœ¨ç±»é‡Œé¢ï¼‰ ==================
    # ä»¥ä¸‹æ–¹æ³•éœ€è¦æ·»åŠ åˆ° CostPredictionSystem ç±»ä¸­ï¼Œå»ºè®®æ”¾åœ¨ç±»çš„æœ«å°¾

    def load_comprehensive_indicators_status(self):
        """ä¸ºå½“å‰æ¨¡å¼åŠ è½½ç»¼åˆæŒ‡æ ‡çŠ¶æ€"""
        self.comprehensive_indicators_status = get_comprehensive_indicators_status(self.mode)
        self.prediction_method_availability = check_prediction_method_availability(self.mode)
        self.logger.info(f"ç»¼åˆæŒ‡æ ‡çŠ¶æ€åŠ è½½å®Œæˆ for mode: {self.mode}")

    def is_prediction_method_enabled(self, method_key):
        """
        æ£€æŸ¥æŒ‡å®šé¢„æµ‹æ–¹æ³•æ˜¯å¦å¯ç”¨
        
        Args:
            method_key (str): é¢„æµ‹æ–¹æ³•é”®å (å¦‚ 'ml_prediction_raw', 'ratio_method_final')
            
        Returns:
            bool: æ˜¯å¦å¯ç”¨
        """
        if not hasattr(self, 'prediction_method_availability'):
            self.load_comprehensive_indicators_status()
        
        return self.prediction_method_availability.get(method_key, {}).get('enabled', False)

    def get_disabled_prediction_info(self, method_key):
        """
        è·å–è¢«ç¦ç”¨çš„é¢„æµ‹æ–¹æ³•çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            method_key (str): é¢„æµ‹æ–¹æ³•é”®å
            
        Returns:
            dict: ç¦ç”¨ä¿¡æ¯
        """
        if not hasattr(self, 'prediction_method_availability'):
            self.load_comprehensive_indicators_status()
        
        method_info = self.prediction_method_availability.get(method_key, {})
        
        return {
            'status': 'disabled',
            'message': f"ğŸš« å·²ç¦ç”¨ - è‹¥éœ€å¯ç”¨è¯·åˆ°æ•°æ®ç®¡ç†æ¨¡å—ä¿®æ”¹ã€Œ{method_info.get('name', 'æœªçŸ¥æŒ‡æ ‡')}ã€çŠ¶æ€",
            'indicator_code': method_info.get('indicator_code', ''),
            'indicator_name': method_info.get('name', '')
        }

    # ================== æ–°å¢æ–¹æ³•å¼€å§‹ ==================
    
    def check_algorithm_execution_capability(self):
        """
        æ£€æŸ¥ç®—æ³•å±‚çš„æ‰§è¡Œèƒ½åŠ›
        
        Returns:
            dict: ç®—æ³•æ‰§è¡Œèƒ½åŠ›çŠ¶æ€
        """
        if not self.algorithm_status_loaded:
            self.load_algorithm_configs()
        
        enabled_algorithms = list(self.enabled_algorithms.keys())
        total_algorithms = len(self.algorithm_configs)
        enabled_count = len(enabled_algorithms)
        
        return {
            'can_execute_ai': enabled_count > 0,
            'enabled_algorithms': enabled_algorithms,
            'enabled_count': enabled_count,
            'total_count': total_algorithms,
            'status': 'available' if enabled_count > 0 else 'unavailable',
            'message': f'æœ‰{enabled_count}ä¸ªç®—æ³•å¯ç”¨' if enabled_count > 0 else 'æ‰€æœ‰AIé¢„æµ‹ç®—æ³•å·²åœç”¨'
        }

    def check_result_display_permission(self, method_key):
        """
        æ£€æŸ¥ç»¼åˆæŒ‡æ ‡å±‚çš„æ˜¾ç¤ºæƒé™
        
        Args:
            method_key (str): é¢„æµ‹æ–¹æ³•é”®å
            
        Returns:
            dict: æ˜¾ç¤ºæƒé™çŠ¶æ€
        """
        if not hasattr(self, 'prediction_method_availability'):
            self.load_comprehensive_indicators_status()
        
        method_info = self.prediction_method_availability.get(method_key, {})
        enabled = method_info.get('enabled', False)
        
        return {
            'can_display': enabled,
            'status': 'enabled' if enabled else 'disabled',
            'indicator_name': method_info.get('name', ''),
            'indicator_code': method_info.get('indicator_code', ''),
            'message': 'å…è®¸æ˜¾ç¤º' if enabled else f"æ˜¾ç¤ºå·²ç¦ç”¨ - ã€Œ{method_info.get('name', 'æœªçŸ¥æŒ‡æ ‡')}ã€"
        }

    def can_execute_ai_prediction(self):
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡ŒAIé¢„æµ‹è®¡ç®—
        
        Returns:
            bool: æ˜¯å¦å¯ä»¥æ‰§è¡ŒAIé¢„æµ‹
        """
        algo_capability = self.check_algorithm_execution_capability()
        return algo_capability['can_execute_ai'] and self.is_trained and len(self.models) > 0

    def can_execute_ratio_prediction(self):
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œæ¯”ç‡æ³•é¢„æµ‹è®¡ç®—
        
        Returns:
            bool: æ˜¯å¦å¯ä»¥æ‰§è¡Œæ¯”ç‡æ³•é¢„æµ‹
        """
        # æ¯”ç‡æ³•é¢„æµ‹åªéœ€è¦å†å²æ•°æ®å’Œè§„åˆ™ï¼Œä¸ä¾èµ–ç®—æ³•é…ç½®
        return (self.is_trained and 
                self.df_historical is not None and 
                len(self.df_historical) > 0 and 
                bool(self.global_rules))

    def get_combined_prediction_status(self, method_key):
        """
        è·å–é¢„æµ‹æ–¹æ³•çš„ç»¼åˆçŠ¶æ€ï¼ˆç®—æ³•èƒ½åŠ› + æ˜¾ç¤ºæƒé™ï¼‰
        
        Args:
            method_key (str): é¢„æµ‹æ–¹æ³•é”®å
            
        Returns:
            dict: ç»¼åˆçŠ¶æ€ä¿¡æ¯
        """
        display_permission = self.check_result_display_permission(method_key)
        
        if method_key.startswith('ml_') or 'AIé¢„æµ‹' in method_key:
            # AIé¢„æµ‹æ–¹æ³•
            algo_capability = self.check_algorithm_execution_capability()
            can_execute = self.can_execute_ai_prediction()
            
            return {
                'method_key': method_key,
                'can_execute': can_execute,
                'can_display': display_permission['can_display'],
                'execution_status': algo_capability,
                'display_status': display_permission,
                'final_status': self._determine_final_status(can_execute, display_permission['can_display'], 'ai')
            }
        elif method_key.startswith('ratio_') or 'æ¯”ç‡æ³•' in method_key:
            # æ¯”ç‡æ³•é¢„æµ‹æ–¹æ³•
            can_execute = self.can_execute_ratio_prediction()
            
            return {
                'method_key': method_key,
                'can_execute': can_execute,
                'can_display': display_permission['can_display'],
                'execution_status': {'can_execute': can_execute, 'message': 'æ¯”ç‡æ³•é¢„æµ‹å¯ç”¨' if can_execute else 'æ¯”ç‡æ³•é¢„æµ‹ä¸å¯ç”¨'},
                'display_status': display_permission,
                'final_status': self._determine_final_status(can_execute, display_permission['can_display'], 'ratio')
            }
        else:
            return {
                'method_key': method_key,
                'can_execute': False,
                'can_display': False,
                'final_status': 'unknown_method'
            }

    def _determine_final_status(self, can_execute, can_display, method_type):
        """
        ç¡®å®šé¢„æµ‹æ–¹æ³•çš„æœ€ç»ˆçŠ¶æ€
        
        Args:
            can_execute (bool): æ˜¯å¦å¯ä»¥æ‰§è¡Œ
            can_display (bool): æ˜¯å¦å¯ä»¥æ˜¾ç¤º
            method_type (str): æ–¹æ³•ç±»å‹ ('ai' æˆ– 'ratio')
            
        Returns:
            str: æœ€ç»ˆçŠ¶æ€æè¿°
        """
        if can_execute and can_display:
            return 'fully_available'
        elif can_execute and not can_display:
            return 'execute_only'  # å¯ä»¥æ‰§è¡Œä½†ä¸æ˜¾ç¤º
        elif not can_execute and can_display:
            return 'display_error'  # æƒ³æ˜¾ç¤ºä½†æ— æ³•æ‰§è¡Œ
        else:
            return 'fully_disabled'  # æ—¢ä¸èƒ½æ‰§è¡Œä¹Ÿä¸èƒ½æ˜¾ç¤º

    def get_best_algorithm_info_with_params(self, ml_predictions, ratio_prediction_value):
        """
        è·å–æœ€ä½³ç®—æ³•ä¿¡æ¯åŠå…¶å‚æ•°é…ç½®
        
        Args:
            ml_predictions (dict): æœºå™¨å­¦ä¹ é¢„æµ‹ç»“æœ
            ratio_prediction_value: æ¯”ç‡æ³•é¢„æµ‹å€¼
            
        Returns:
            dict: æœ€ä½³ç®—æ³•çš„è¯¦ç»†ä¿¡æ¯å’Œå‚æ•°
        """
        if not ratio_prediction_value or not ml_predictions:
            return {
                "best_algorithm_name": None,
                "best_prediction_value": None,
                "algorithm_parameters": {},
                "parameter_details": {},
                "selection_reason": "æ— æœ‰æ•ˆé¢„æµ‹ç»“æœç”¨äºæ¯”è¾ƒ"
            }
        
        best_algorithm_key = None
        min_diff_to_ratio = float('inf')
        best_prediction_value = None
        
        # æ‰¾åˆ°æœ€æ¥è¿‘æ¯”ç‡æ³•çš„ç®—æ³•
        for model_key, prediction_value in ml_predictions.items():
            # è·³è¿‡ç‰¹æ®Šé”®å’Œåœç”¨ç®—æ³•
            if (model_key in ['é›†æˆå¹³å‡é¢„æµ‹', '_algorithm_status'] or 
                prediction_value is None or 
                isinstance(prediction_value, dict)):
                continue
                
            # ç¡®ä¿è¿™æ˜¯ä¸€ä¸ªå¯ç”¨çš„ç®—æ³•ä¸”æœ‰æ•°å€¼é¢„æµ‹ç»“æœ
            if isinstance(prediction_value, (int, float)):
                diff = abs(prediction_value - ratio_prediction_value)
                if diff < min_diff_to_ratio:
                    min_diff_to_ratio = diff
                    best_algorithm_key = model_key
                    best_prediction_value = prediction_value
        
        if not best_algorithm_key:
            return {
                "best_algorithm_name": None,
                "best_prediction_value": None,
                "algorithm_parameters": {},
                "parameter_details": {},
                "selection_reason": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä½³ç®—æ³•"
            }
        
        # è·å–ç®—æ³•çš„ä¸­æ–‡åç§°
        algorithm_name_mapping = {
            "å²­å›å½’ (RidgeCV)": "å²­å›å½’",
            "å†³ç­–æ ‘ (Decision Tree)": "å†³ç­–æ ‘", 
            "éšæœºæ£®æ— (Random Forest)": "éšæœºæ£®æ—",
            "æ”¯æŒå‘é‡å›å½’ (SVR)": "æ”¯æŒå‘é‡å›å½’",
            "ç¥ç»ç½‘ç»œ (MLPRegressor)": "ç¥ç»ç½‘ç»œ"
        }
        
        algorithm_chinese_name = algorithm_name_mapping.get(best_algorithm_key, best_algorithm_key)
        
        # è·å–ç®—æ³•çš„å‚æ•°é…ç½®
        algorithm_params = self.get_algorithm_parameters_parsed(algorithm_chinese_name)
        
        # è·å–å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ä¸­æ–‡åç§°ã€å»ºè®®èŒƒå›´ç­‰ï¼‰
        parameter_details = {}
        if self.parameters_loaded:
            raw_params = self.get_algorithm_parameters_raw(algorithm_chinese_name)
            for param_name, param_info in raw_params.items():
                parameter_details[param_name] = {
                    "chinese_name": param_info.get("value", param_name),  # ä¿®æ­£ï¼šåº”è¯¥ä»æ•°æ®åº“å­—æ®µè·å–ä¸­æ–‡å
                    "current_value": param_info.get("value"),
                    "type": param_info.get("type"),
                    "parsed_value": algorithm_params.get(param_name),
                    "suggested_range": "",  # æš‚æ—¶ä¸ºç©ºï¼Œéœ€è¦ä»æ•°æ®åº“è·å–
                    "adjustment_tips": ""   # æš‚æ—¶ä¸ºç©ºï¼Œéœ€è¦ä»æ•°æ®åº“è·å–
                }
        
        # è®¡ç®—ä¸æ¯”ç‡æ³•çš„å·®å¼‚ç™¾åˆ†æ¯”
        diff_percentage = (min_diff_to_ratio / ratio_prediction_value) * 100 if ratio_prediction_value > 0 else 0
        
        return {
            "best_algorithm_name": best_algorithm_key,
            "best_algorithm_chinese_name": algorithm_chinese_name,
            "best_prediction_value": best_prediction_value,
            "algorithm_parameters": algorithm_params,
            "parameter_details": parameter_details,
            "selection_reason": f"ä¸æ¯”ç‡æ³•é¢„æµ‹å€¼æœ€æ¥è¿‘ï¼Œå·®å¼‚: {min_diff_to_ratio:,.2f}å…ƒ ({diff_percentage:.2f}%)",
            "difference_to_ratio": min_diff_to_ratio,
            "difference_percentage": diff_percentage
        }

    def format_best_algorithm_params_for_report(self, best_algorithm_info):
        """
        æ ¼å¼åŒ–æœ€ä½³ç®—æ³•å‚æ•°ä¿¡æ¯ï¼Œç”¨äºæŠ¥è¡¨æ˜¾ç¤º
        
        Args:
            best_algorithm_info (dict): æœ€ä½³ç®—æ³•ä¿¡æ¯
            
        Returns:
            dict: æ ¼å¼åŒ–åçš„ä¿¡æ¯ï¼Œé€‚åˆåœ¨æŠ¥è¡¨ä¸­æ˜¾ç¤º
        """
        if not best_algorithm_info or not best_algorithm_info.get("best_algorithm_name"):
            return {
                "ç®—æ³•åç§°": "æœªç¡®å®š",
                "å‚æ•°é…ç½®": "æ— ",
                "é€‰æ‹©åŸå› ": "æ— æœ‰æ•ˆç®—æ³•ç»“æœ"
            }
        
        # æ ¼å¼åŒ–å‚æ•°åˆ—è¡¨
        formatted_params = []
        parameter_details = best_algorithm_info.get("parameter_details", {})
        
        for param_name, param_info in parameter_details.items():
            chinese_name = param_info.get("chinese_name", param_name)
            current_value = param_info.get("current_value", "æœªçŸ¥")
            param_type = param_info.get("type", "")
            
            formatted_params.append({
                "å‚æ•°å": param_name,
                "ä¸­æ–‡å": chinese_name,
                "å½“å‰å€¼": current_value,
                "ç±»å‹": param_type,
                "å»ºè®®èŒƒå›´": param_info.get("suggested_range", ""),
                "è°ƒä¼˜æç¤º": param_info.get("adjustment_tips", "")
            })
        
        return {
            "ç®—æ³•åç§°": best_algorithm_info.get("best_algorithm_name", ""),
            "ç®—æ³•ä¸­æ–‡å": best_algorithm_info.get("best_algorithm_chinese_name", ""),
            "é¢„æµ‹å€¼": best_algorithm_info.get("best_prediction_value", 0),
            "é€‰æ‹©åŸå› ": best_algorithm_info.get("selection_reason", ""),
            "ä¸æ¯”ç‡æ³•å·®å¼‚": best_algorithm_info.get("difference_to_ratio", 0),
            "å·®å¼‚ç™¾åˆ†æ¯”": best_algorithm_info.get("difference_percentage", 0),
            "å‚æ•°é…ç½®": formatted_params,
            "å‚æ•°æ•°é‡": len(formatted_params)
        }
    
# ================== ç‹¬ç«‹å‡½æ•°ï¼ˆæ”¾åœ¨æ–‡ä»¶æœ«å°¾ï¼ŒCostPredictionSystem ç±»å¤–é¢ï¼‰ ==================

def get_comprehensive_indicators_status(mode='steel_cage'):
    """
    æŸ¥è¯¢comprehensive_indicatorsè¡¨ä¸­æŒ‡å®šæ¨¡å¼çš„çŠ¶æ€ä¿¡æ¯
    
    Args:
        mode (str): æ–½å·¥æ¨¡å¼ ('steel_cage' æˆ– 'steel_lining')
    
    Returns:
        dict: {æŒ‡æ ‡ç¼–ç : çŠ¶æ€ä¿¡æ¯} çš„å­—å…¸
    """
    conn = None
    try:
        conn = get_connection()
        cursor = conn.cursor(dictionary=True)
        
        query = """
        SELECT code, name, calculation_method, indicator_type, status, calculation_logic
        FROM comprehensive_indicators 
        WHERE construction_mode = %s
        ORDER BY id
        """
        
        cursor.execute(query, (mode,))
        results = cursor.fetchall()
        
        status_dict = {}
        for row in results:
            status_dict[row['code']] = {
                'name': row['name'],
                'calculation_method': row['calculation_method'],
                'indicator_type': row['indicator_type'], 
                'status': row['status'],
                'calculation_logic': row.get('calculation_logic', ''),
                'enabled': row['status'] == 'enabled'
            }
        
        # ä½¿ç”¨ logging æ¨¡å—è€Œä¸æ˜¯ logger
        logging.info(f"æˆåŠŸæŸ¥è¯¢åˆ° {len(status_dict)} ä¸ªç»¼åˆæŒ‡æ ‡çš„çŠ¶æ€ä¿¡æ¯ for mode: {mode}")
        return status_dict
        
    except mysql.connector.Error as e:
        logging.error(f"æŸ¥è¯¢ç»¼åˆæŒ‡æ ‡çŠ¶æ€å¤±è´¥ for mode {mode}: {e}")
        return {}
    except Exception as e:
        logging.error(f"æŸ¥è¯¢ç»¼åˆæŒ‡æ ‡çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯ for mode {mode}: {e}")
        return {}
    finally:
        if conn:
            conn.close()

def check_prediction_method_availability(mode='steel_cage'):
    """
    æ£€æŸ¥æŒ‡å®šæ¨¡å¼ä¸‹å„ç§é¢„æµ‹æ–¹æ³•çš„å¯ç”¨æ€§
    
    Args:
        mode (str): æ–½å·¥æ¨¡å¼
        
    Returns:
        dict: å„é¢„æµ‹æ–¹æ³•çš„å¯ç”¨æ€§çŠ¶æ€
    """
    indicators_status = get_comprehensive_indicators_status(mode)
    
    # æ ¹æ®æ¨¡å¼å®šä¹‰æŒ‡æ ‡ç¼–ç æ˜ å°„
    if mode == 'steel_cage':
        indicator_mapping = {
            'ml_prediction_raw': 'FU-ZJ-ML-RAW',
            'ml_prediction_final': 'FU-ZJ-ML-FINAL', 
            'ratio_method_raw': 'FU-ZJ-RATIO-RAW',
            'ratio_method_final': 'FU-ZJ-RATIO-FINAL'
        }
    elif mode == 'steel_lining':
        indicator_mapping = {
            'ml_prediction_raw': 'FU-ZJ-GCL-ML-RAW',
            'ml_prediction_final': 'FU-ZJ-GCL-ML-FINAL',
            'ratio_method_raw': 'FU-ZJ-GCL-RATIO-RAW', 
            'ratio_method_final': 'FU-ZJ-GCL-RATIO-FINAL'
        }
    else:
        logging.warning(f"æœªæ”¯æŒçš„æ¨¡å¼: {mode}")
        return {}
    
    availability = {}
    for method_key, indicator_code in indicator_mapping.items():
        indicator_info = indicators_status.get(indicator_code, {})
        availability[method_key] = {
            'enabled': indicator_info.get('enabled', False),
            'status': indicator_info.get('status', 'unknown'),
            'name': indicator_info.get('name', ''),
            'indicator_code': indicator_code
        }
    
    logging.info(f"é¢„æµ‹æ–¹æ³•å¯ç”¨æ€§æ£€æŸ¥å®Œæˆ for mode {mode}: {availability}")
    return availability

def get_prediction_status_summary(mode='steel_cage'):
    """
    è·å–é¢„æµ‹çŠ¶æ€çš„æ±‡æ€»ä¿¡æ¯ï¼Œç”¨äºç•Œé¢æ˜¾ç¤º
    
    Args:
        mode (str): æ–½å·¥æ¨¡å¼
        
    Returns:
        dict: çŠ¶æ€æ±‡æ€»ä¿¡æ¯
    """
    availability = check_prediction_method_availability(mode)
    
    enabled_methods = []
    disabled_methods = []
    
    method_display_names = {
        'ml_prediction_raw': 'AIé¢„æµ‹-åŸå§‹å€¼',
        'ml_prediction_final': 'AIé¢„æµ‹-æœ€ç»ˆå€¼',
        'ratio_method_raw': 'æ¯”ç‡æ³•-åŸå§‹å€¼', 
        'ratio_method_final': 'æ¯”ç‡æ³•-æœ€ç»ˆå€¼'
    }
    
    for method_key, status_info in availability.items():
        display_name = method_display_names.get(method_key, method_key)
        
        if status_info['enabled']:
            enabled_methods.append({
                'key': method_key,
                'display_name': display_name,
                'indicator_name': status_info['name']
            })
        else:
            disabled_methods.append({
                'key': method_key,
                'display_name': display_name,
                'indicator_name': status_info['name']
            })
    
    return {
        'total_methods': len(availability),
        'enabled_count': len(enabled_methods),
        'disabled_count': len(disabled_methods),
        'enabled_methods': enabled_methods,
        'disabled_methods': disabled_methods,
        'all_enabled': len(disabled_methods) == 0,
        'all_disabled': len(enabled_methods) == 0,
        'partial_available': len(enabled_methods) > 0 and len(disabled_methods) > 0
    }

# ==================== æ›´æ–°åçš„é…ç½®æ–‡ä»¶ ====================
# modules/pricePrediction/config.py

# ==================== MySQLé…ç½® ====================
# MySQLæ•°æ®åº“è¿æ¥é…ç½®åœ¨å•ç‹¬çš„æ¨¡å—ä¸­å®šä¹‰
# è¿™é‡Œåªå®šä¹‰è¡¨åå’Œåˆ—æ˜ å°„

